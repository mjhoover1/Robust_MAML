{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Torchmeta_MAML.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMzsb5BIHrxQpGQs2I0xrL8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mjhoover1/Robust_MAML/blob/main/Torchmeta_MAML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjVEiL3Gzr__"
      },
      "source": [
        "First, install the library torchmeta:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqIO-iA4zrzo",
        "outputId": "e371dcad-bdde-41e1-c7ff-074aa43316c9"
      },
      "source": [
        "!pip install torchmeta"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmeta\n",
            "  Downloading torchmeta-1.8.0-py3-none-any.whl (210 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▋                              | 10 kB 19.6 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 20 kB 22.3 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 30 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 40 kB 25.8 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 51 kB 26.8 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 61 kB 28.8 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 71 kB 24.7 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 81 kB 26.0 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 92 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 102 kB 26.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 112 kB 26.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 122 kB 26.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 133 kB 26.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 143 kB 26.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 153 kB 26.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 163 kB 26.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 174 kB 26.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 184 kB 26.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 194 kB 26.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 204 kB 26.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 210 kB 26.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<1.10.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from torchmeta) (1.9.0+cu102)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from torchmeta) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from torchmeta) (3.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchmeta) (2.23.0)\n",
            "Requirement already satisfied: Pillow>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from torchmeta) (7.1.2)\n",
            "Requirement already satisfied: tqdm>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from torchmeta) (4.62.2)\n",
            "Collecting ordered-set\n",
            "  Downloading ordered-set-4.0.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: torchvision<0.11.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from torchmeta) (0.10.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<1.10.0,>=1.4.0->torchmeta) (3.7.4.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->torchmeta) (1.5.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchmeta) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchmeta) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchmeta) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchmeta) (3.0.4)\n",
            "Building wheels for collected packages: ordered-set\n",
            "  Building wheel for ordered-set (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ordered-set: filename=ordered_set-4.0.2-py2.py3-none-any.whl size=8219 sha256=9b59dd2ce40b67cb2d1ddc903c8952a4ae448e6ca254ebd971943a125ed41473\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/2b/f6/26e9f84153c25050fe7c09e88f8e32a6be3c7034a38c418319\n",
            "Successfully built ordered-set\n",
            "Installing collected packages: ordered-set, torchmeta\n",
            "Successfully installed ordered-set-4.0.2 torchmeta-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1FxOV1Wzi8W"
      },
      "source": [
        "MAML example using torchmeta: Here is the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cU0HzCzozfJy"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchmeta.modules import (MetaModule, MetaSequential, MetaConv2d, MetaBatchNorm2d, MetaLinear)\n",
        "\n",
        "def conv3x3(in_channels, out_channels):\n",
        "  return MetaSequential(\n",
        "      MetaConv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "      MetaBatchNorm2d(out_channels, momentum=1., track_running_stats=False),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(2)\n",
        "  )\n",
        "\n",
        "class ConvolutionalNeuralNetwork(MetaModule):\n",
        "  def __init__(self, in_channels, out_features, hidden_size=64):\n",
        "    super(ConvolutionalNeuralNetwork, self).__init__()\n",
        "    self.in_channels = in_channels\n",
        "    self.out_features = out_features\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    self.features = MetaSequential(\n",
        "        conv3x3(in_channels, hidden_size),\n",
        "        conv3x3(hidden_size, hidden_size),\n",
        "        conv3x3(hidden_size, hidden_size),\n",
        "        conv3x3(hidden_size, hidden_size)\n",
        "    )\n",
        "\n",
        "    self.classifier = MetaLinear(hidden_size, out_features)\n",
        "\n",
        "  def forward(self, inputs, params=None):\n",
        "    features = self.features(inputs, params=self.get_subdict(params, 'features'))\n",
        "    features = features.view((features.size(0), -1))\n",
        "    logits = self.classifier(features, params=self.get_subdict(params, 'classifier'))\n",
        "    return logits"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCMAJuItz01I"
      },
      "source": [
        "Here are the utility functions for the MAML example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm19YeABzltk"
      },
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "def get_accuracy(logits, targets):\n",
        "    \"\"\"Compute the accuracy (after adaptation) of MAML on the test/query points\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    logits : `torch.FloatTensor` instance\n",
        "        Outputs/logits of the model on the query points. This tensor has shape\n",
        "        `(num_examples, num_classes)`.\n",
        "    targets : `torch.LongTensor` instance\n",
        "        A tensor containing the targets of the query points. This tensor has \n",
        "        shape `(num_examples,)`.\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    accuracy : `torch.FloatTensor` instance\n",
        "        Mean accuracy on the query points\n",
        "    \"\"\"\n",
        "    _, predictions = torch.max(logits, dim=-1)\n",
        "    return torch.mean(predictions.eq(targets).float())"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_JG_TfRz24C"
      },
      "source": [
        "Here is the training for the MAML example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "la0oxzmPz5SC",
        "outputId": "6ebad082-77b7-4da2-f5b4-06afdb3e017c"
      },
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "device = get_default_device()\n",
        "device"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au6BGjdoz9AF"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "\n",
        "from torchmeta.datasets.helpers import omniglot\n",
        "from torchmeta.utils.data import BatchMetaDataLoader\n",
        "from torchmeta.utils.gradient_based import gradient_update_parameters\n",
        "\n",
        "logger = logging.getLogger('example 1')\n",
        "\n",
        "def train(folder, device, num_shots=5, num_ways=5, batch_size=16, num_batches=100, num_workers=1, hidden_size=64, step_size=.4, first_order=True, output_folder=None):\n",
        "  dataset = omniglot(folder, shots=num_shots, ways=num_ways, shuffle=True, test_shots=15, \n",
        "                     meta_train=True, download=True)\n",
        "  dataloader = BatchMetaDataLoader(dataset, batch_size=batch_size, shuffle=True, \n",
        "                                   num_workers=num_workers)\n",
        "  model = ConvolutionalNeuralNetwork(1, num_ways, hidden_size=hidden_size)\n",
        "  model.to(device=device)\n",
        "  model.train()\n",
        "  meta_optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "  # Training loop\n",
        "  with tqdm(dataloader, total=num_batches) as pbar:\n",
        "    for batch_idx, batch in enumerate(pbar):\n",
        "\n",
        "      model.zero_grad()\n",
        "\n",
        "      train_inputs, train_targets = batch['train']\n",
        "      train_inputs = train_inputs.to(device=device)\n",
        "      train_targets = train_targets.to(device=device)\n",
        "\n",
        "      test_inputs, test_targets = batch['test']\n",
        "      test_inputs = test_inputs.to(device=device)\n",
        "      test_targets = test_targets.to(device=device)\n",
        "      \n",
        "      outer_loss = torch.tensor(0., device=device)\n",
        "      accuracy = torch.tensor(0., device=device)\n",
        "      for task_idx, (train_input, train_target, test_input,\n",
        "                     test_target) in enumerate(zip(train_inputs, train_targets,\n",
        "                                                   test_inputs, test_targets)):\n",
        "                       train_logit = model(train_input)\n",
        "                       inner_loss = F.cross_entropy(train_logit, train_target)\n",
        "\n",
        "                       model.zero_grad()\n",
        "                       params = gradient_update_parameters(model,\n",
        "                                                           inner_loss,\n",
        "                                                           step_size=step_size,\n",
        "                                                           first_order=first_order)\n",
        "                       \n",
        "                       test_logit = model(test_input, params=params)\n",
        "                       outer_loss += F.cross_entropy(test_logit, test_target)\n",
        "\n",
        "                       with torch.no_grad():\n",
        "                         accuracy += get_accuracy(test_logit, test_target)\n",
        "      outer_loss.div_(batch_size)\n",
        "      accuracy.div_(batch_size)\n",
        "\n",
        "      outer_loss.backward()\n",
        "      meta_optimizer.step()\n",
        "\n",
        "      pbar.set_postfix(accuracy='{0:.4f}'.format(accuracy.item()))\n",
        "      if batch_idx >= num_batches: break\n",
        "\n",
        "  # Save model\n",
        "  if output_folder is not None:\n",
        "    filename = os.path.join(output_folder, 'maml_omniglot_{0}shot_{1}way.th'.format(\n",
        "        num_shots, num_ways))\n",
        "    with open(filename, 'wb') as f:\n",
        "      state_dict = model.state_dict()\n",
        "      torch.save(state_dict, f)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9fyxHJ5z-V6"
      },
      "source": [
        "# train(folder, device, num_shots=5, num_ways=5, batch_size=16, num_batches=100, num_workers=1, hidden_size=64, step_size=.4, first_order=True, output_folder=None):\n",
        "train(\"data\", device)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
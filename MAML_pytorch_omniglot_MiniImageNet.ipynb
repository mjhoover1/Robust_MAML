{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "MAML_pytorch_omniglot-MiniImageNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mjhoover1/Robust_MAML/blob/main/MAML_pytorch_omniglot_MiniImageNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-qBD8OYob3M"
      },
      "source": [
        "# ATM you must physically download the zip files because the commands below download a HTML file\n",
        "# !wget https://github.com/brendenlake/omniglot/blob/master/python/images_evaluation.zip\n",
        "# !wget https://github.com/brendenlake/omniglot/blob/master/python/images_background.zip\n",
        "# Download the files from Github and upload the zip files manually"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGx_E6n30cg2"
      },
      "source": [
        "# Source code repo: https://github.com/oscarknagg/few-shot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pWfPNfXpUaX"
      },
      "source": [
        "# Make necessary folders that are in the repo\n",
        "mkdir(\"few_shot\")\n",
        "mkdir(\"scripts\")\n",
        "mkdir(\"experiments\")"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7SZhMCy0wcG"
      },
      "source": [
        "# Below write the necessary files needed for training and testing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkNELcM2OzSG",
        "outputId": "ecf85454-2d71-4da7-c315-5944d59faf45"
      },
      "source": [
        "%%writefile config.py\n",
        "import os\n",
        "\n",
        "\n",
        "PATH = os.getcwd()\n",
        "\n",
        "DATA_PATH = os.getcwd()\n",
        "\n",
        "EPSILON = 1e-8\n",
        "\n",
        "if DATA_PATH is None:\n",
        "    raise Exception('Configure your data folder location in config.py before continuing!')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUD3ri6VLhuv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b0c927b-825b-44ec-e287-42e0d4b2ba02"
      },
      "source": [
        "# few_shot/utils.py\n",
        "%%writefile few_shot/utils.py\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "from typing import Tuple, List\n",
        "\n",
        "from config import EPSILON, PATH\n",
        "\n",
        "\n",
        "def mkdir(dir):\n",
        "    \"\"\"Create a directory, ignoring exceptions\n",
        "    # Arguments:\n",
        "        dir: Path of directory to create\n",
        "    \"\"\"\n",
        "    try:\n",
        "        os.mkdir(dir)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "def rmdir(dir):\n",
        "    \"\"\"Recursively remove a directory and contents, ignoring exceptions\n",
        "   # Arguments:\n",
        "       dir: Path of directory to recursively remove\n",
        "   \"\"\"\n",
        "    try:\n",
        "        shutil.rmtree(dir)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "def setup_dirs():\n",
        "    \"\"\"Creates directories for this project.\"\"\"\n",
        "    mkdir(PATH + '/logs/')\n",
        "    mkdir(PATH + '/logs/proto_nets')\n",
        "    mkdir(PATH + '/logs/matching_nets')\n",
        "    mkdir(PATH + '/logs/maml')\n",
        "    mkdir(PATH + '/models/')\n",
        "    mkdir(PATH + '/models/proto_nets')\n",
        "    mkdir(PATH + '/models/matching_nets')\n",
        "    mkdir(PATH + '/models/maml')\n",
        "\n",
        "\n",
        "def pairwise_distances(x: torch.Tensor,\n",
        "                       y: torch.Tensor,\n",
        "                       matching_fn: str) -> torch.Tensor:\n",
        "    \"\"\"Efficiently calculate pairwise distances (or other similarity scores) between\n",
        "    two sets of samples.\n",
        "    # Arguments\n",
        "        x: Query samples. A tensor of shape (n_x, d) where d is the embedding dimension\n",
        "        y: Class prototypes. A tensor of shape (n_y, d) where d is the embedding dimension\n",
        "        matching_fn: Distance metric/similarity score to compute between samples\n",
        "    \"\"\"\n",
        "    n_x = x.shape[0]\n",
        "    n_y = y.shape[0]\n",
        "\n",
        "    if matching_fn == 'l2':\n",
        "        distances = (\n",
        "                x.unsqueeze(1).expand(n_x, n_y, -1) -\n",
        "                y.unsqueeze(0).expand(n_x, n_y, -1)\n",
        "        ).pow(2).sum(dim=2)\n",
        "        return distances\n",
        "    elif matching_fn == 'cosine':\n",
        "        normalised_x = x / (x.pow(2).sum(dim=1, keepdim=True).sqrt() + EPSILON)\n",
        "        normalised_y = y / (y.pow(2).sum(dim=1, keepdim=True).sqrt() + EPSILON)\n",
        "\n",
        "        expanded_x = normalised_x.unsqueeze(1).expand(n_x, n_y, -1)\n",
        "        expanded_y = normalised_y.unsqueeze(0).expand(n_x, n_y, -1)\n",
        "\n",
        "        cosine_similarities = (expanded_x * expanded_y).sum(dim=2)\n",
        "        return 1 - cosine_similarities\n",
        "    elif matching_fn == 'dot':\n",
        "        expanded_x = x.unsqueeze(1).expand(n_x, n_y, -1)\n",
        "        expanded_y = y.unsqueeze(0).expand(n_x, n_y, -1)\n",
        "\n",
        "        return -(expanded_x * expanded_y).sum(dim=2)\n",
        "    else:\n",
        "        raise(ValueError('Unsupported similarity function'))\n",
        "\n",
        "\n",
        "def copy_weights(from_model: torch.nn.Module, to_model: torch.nn.Module):\n",
        "    \"\"\"Copies the weights from one model to another model.\n",
        "    # Arguments:\n",
        "        from_model: Model from which to source weights\n",
        "        to_model: Model which will receive weights\n",
        "    \"\"\"\n",
        "    if not from_model.__class__ == to_model.__class__:\n",
        "        raise(ValueError(\"Models don't have the same architecture!\"))\n",
        "\n",
        "    for m_from, m_to in zip(from_model.modules(), to_model.modules()):\n",
        "        is_linear = isinstance(m_to, torch.nn.Linear)\n",
        "        is_conv = isinstance(m_to, torch.nn.Conv2d)\n",
        "        is_bn = isinstance(m_to, torch.nn.BatchNorm2d)\n",
        "        if is_linear or is_conv or is_bn:\n",
        "            m_to.weight.data = m_from.weight.data.clone()\n",
        "            if m_to.bias is not None:\n",
        "                m_to.bias.data = m_from.bias.data.clone()\n",
        "\n",
        "\n",
        "def autograd_graph(tensor: torch.Tensor) -> Tuple[\n",
        "            List[torch.autograd.Function],\n",
        "            List[Tuple[torch.autograd.Function, torch.autograd.Function]]\n",
        "        ]:\n",
        "    \"\"\"Recursively retrieves the autograd graph for a particular tensor.\n",
        "    # Arguments\n",
        "        tensor: The Tensor to retrieve the autograd graph for\n",
        "    # Returns\n",
        "        nodes: List of torch.autograd.Functions that are the nodes of the autograd graph\n",
        "        edges: List of (Function, Function) tuples that are the edges between the nodes of the autograd graph\n",
        "    \"\"\"\n",
        "    nodes, edges = list(), list()\n",
        "\n",
        "    def _add_nodes(tensor):\n",
        "        if tensor not in nodes:\n",
        "            nodes.append(tensor)\n",
        "\n",
        "            if hasattr(tensor, 'next_functions'):\n",
        "                for f in tensor.next_functions:\n",
        "                    if f[0] is not None:\n",
        "                        edges.append((f[0], tensor))\n",
        "                        _add_nodes(f[0])\n",
        "\n",
        "            if hasattr(tensor, 'saved_tensors'):\n",
        "                for t in tensor.saved_tensors:\n",
        "                    edges.append((t, tensor))\n",
        "                    _add_nodes(t)\n",
        "\n",
        "    _add_nodes(tensor.grad_fn)\n",
        "\n",
        "    return nodes, edges"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing few_shot/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUd3dwHdKZNG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4806c210-b685-46b1-c5e4-851d29182ee5"
      },
      "source": [
        "%%writefile scripts/prepare_omniglot.py\n",
        "\"\"\"\n",
        "Run this script to prepare the Omniglot dataset from the raw Omniglot dataset that is found at\n",
        "https://github.com/brendenlake/omniglot/tree/master/python.\n",
        "This script prepares an enriched version of Omniglot the same as is used in the Matching Networks and Prototypical\n",
        "Networks papers.\n",
        "1. Augment classes with rotations in multiples of 90 degrees.\n",
        "2. Downsize images to 28x28\n",
        "3. Uses background and evaluation sets present in the raw dataset\n",
        "\"\"\"\n",
        "from skimage import io\n",
        "from skimage import transform\n",
        "import zipfile\n",
        "import shutil\n",
        "import os\n",
        "import numpy as np # added\n",
        "\n",
        "from config import DATA_PATH\n",
        "from few_shot.utils import mkdir, rmdir\n",
        "\n",
        "\n",
        "# Parameters\n",
        "dataset_zip_files = ['images_background.zip', 'images_evaluation.zip']\n",
        "raw_omniglot_location = DATA_PATH + '/Omniglot_Raw/'\n",
        "prepared_omniglot_location = DATA_PATH + '/Omniglot/'\n",
        "output_shape = (28, 28)\n",
        "\n",
        "\n",
        "def handle_characters(alphabet_folder, character_folder, rotate):\n",
        "    for root, _, character_images in os.walk(character_folder):\n",
        "        character_name = root.split('/')[-1]\n",
        "        mkdir(f'{alphabet_folder}.{rotate}/{character_name}')\n",
        "        for img_path in character_images:\n",
        "            # print(root+'/'+img_path)\n",
        "            img = io.imread(root+'/'+img_path)\n",
        "            img = transform.rotate(img, angle=rotate)\n",
        "            img = transform.resize(img, output_shape, anti_aliasing=True)\n",
        "            img = (img - img.min()) / (img.max() - img.min())\n",
        "            # print(img.min(), img.max())\n",
        "            # print(f'{alphabet_folder}.{rotate}/{character_name}/{img_path}')\n",
        "            io.imsave(f'{alphabet_folder}.{rotate}/{character_name}/{img_path}', img_as_ubyte(img)) # added 2nd arg\n",
        "            # return\n",
        "\n",
        "\n",
        "def handle_alphabet(folder):\n",
        "    print('{}...'.format(folder.split('/')[-1]))\n",
        "    for rotate in [0, 90, 180, 270]:\n",
        "        # Create new folders for each augmented alphabet\n",
        "        mkdir(f'{folder}.{rotate}')\n",
        "        for root, character_folders, _ in os.walk(folder):\n",
        "            for character_folder in character_folders:\n",
        "                # For each character folder in an alphabet rotate and resize all of the images and save\n",
        "                # to the new folder\n",
        "                handle_characters(folder, root + '/' + character_folder, rotate)\n",
        "                # return\n",
        "\n",
        "    # Delete original alphabet\n",
        "    rmdir(folder)\n",
        "\n",
        "\n",
        "# Clean up previous extraction\n",
        "rmdir(prepared_omniglot_location)\n",
        "mkdir(prepared_omniglot_location)\n",
        "\n",
        "# Unzip dataset\n",
        "for root, _, files in os.walk(raw_omniglot_location):\n",
        "    for f in files:\n",
        "        if f in dataset_zip_files:\n",
        "            print('Unzipping {}...'.format(f))\n",
        "            zip_ref = zipfile.ZipFile(root + f, 'r')\n",
        "            zip_ref.extractall(prepared_omniglot_location)\n",
        "            zip_ref.close()\n",
        "\n",
        "print('Processing background set...')\n",
        "for root, alphabets, _ in os.walk(prepared_omniglot_location + 'images_background/'):\n",
        "    for alphabet in sorted(alphabets):\n",
        "        handle_alphabet(root + alphabet)\n",
        "\n",
        "print('Processing evaluation set...')\n",
        "for root, alphabets, _ in os.walk(prepared_omniglot_location + 'images_evaluation/'):\n",
        "    for alphabet in sorted(alphabets):\n",
        "        handle_alphabet(root + alphabet)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scripts/prepare_omniglot.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iw81NoI9j5KZ",
        "outputId": "84266923-bb52-4ada-d287-c5737df5c785"
      },
      "source": [
        "# few_shot/datasets.py\n",
        "%%writefile few_shot/datasets.py\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from skimage import io\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from config import DATA_PATH\n",
        "\n",
        "\n",
        "class OmniglotDataset(Dataset):\n",
        "    def __init__(self, subset):\n",
        "        \"\"\"Dataset class representing Omniglot dataset\n",
        "        # Arguments:\n",
        "            subset: Whether the dataset represents the background or evaluation set\n",
        "        \"\"\"\n",
        "        if subset not in ('background', 'evaluation'):\n",
        "            raise(ValueError, 'subset must be one of (background, evaluation)')\n",
        "        self.subset = subset\n",
        "\n",
        "        self.df = pd.DataFrame(self.index_subset(self.subset))\n",
        "\n",
        "        # Index of dataframe has direct correspondence to item in dataset\n",
        "        self.df = self.df.assign(id=self.df.index.values)\n",
        "\n",
        "        # Convert arbitrary class names of dataset to ordered 0-(num_speakers - 1) integers\n",
        "        self.unique_characters = sorted(self.df['class_name'].unique())\n",
        "        self.class_name_to_id = {self.unique_characters[i]: i for i in range(self.num_classes())}\n",
        "        self.df = self.df.assign(class_id=self.df['class_name'].apply(lambda c: self.class_name_to_id[c]))\n",
        "\n",
        "        # Create dicts\n",
        "        self.datasetid_to_filepath = self.df.to_dict()['filepath']\n",
        "        self.datasetid_to_class_id = self.df.to_dict()['class_id']\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        instance = io.imread(self.datasetid_to_filepath[item])\n",
        "        # Reindex to channels first format as supported by pytorch\n",
        "        instance = instance[np.newaxis, :, :]\n",
        "\n",
        "        # Normalise to 0-1\n",
        "        instance = (instance - instance.min()) / (instance.max() - instance.min())\n",
        "\n",
        "        label = self.datasetid_to_class_id[item]\n",
        "\n",
        "        return torch.from_numpy(instance), label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def num_classes(self):\n",
        "        return len(self.df['class_name'].unique())\n",
        "\n",
        "    @staticmethod\n",
        "    def index_subset(subset):\n",
        "        \"\"\"Index a subset by looping through all of its files and recording relevant information.\n",
        "        # Arguments\n",
        "            subset: Name of the subset\n",
        "        # Returns\n",
        "            A list of dicts containing information about all the image files in a particular subset of the\n",
        "            Omniglot dataset dataset\n",
        "        \"\"\"\n",
        "        images = []\n",
        "        print('Indexing {}...'.format(subset))\n",
        "        # Quick first pass to find total for tqdm bar\n",
        "        subset_len = 0\n",
        "        for root, folders, files in os.walk(DATA_PATH + '/Omniglot/images_{}/'.format(subset)):\n",
        "            subset_len += len([f for f in files if f.endswith('.png')])\n",
        "\n",
        "        progress_bar = tqdm(total=subset_len)\n",
        "        for root, folders, files in os.walk(DATA_PATH + '/Omniglot/images_{}/'.format(subset)):\n",
        "            if len(files) == 0:\n",
        "                continue\n",
        "\n",
        "            alphabet = root.split('/')[-2]\n",
        "            class_name = '{}.{}'.format(alphabet, root.split('/')[-1])\n",
        "\n",
        "            for f in files:\n",
        "                progress_bar.update(1)\n",
        "                images.append({\n",
        "                    'subset': subset,\n",
        "                    'alphabet': alphabet,\n",
        "                    'class_name': class_name,\n",
        "                    'filepath': os.path.join(root, f)\n",
        "                })\n",
        "\n",
        "        progress_bar.close()\n",
        "        return images\n",
        "\n",
        "\n",
        "class MiniImageNet(Dataset):\n",
        "    def __init__(self, subset):\n",
        "        \"\"\"Dataset class representing miniImageNet dataset\n",
        "        # Arguments:\n",
        "            subset: Whether the dataset represents the background or evaluation set\n",
        "        \"\"\"\n",
        "        if subset not in ('background', 'evaluation'):\n",
        "            raise(ValueError, 'subset must be one of (background, evaluation)')\n",
        "        self.subset = subset\n",
        "\n",
        "        self.df = pd.DataFrame(self.index_subset(self.subset))\n",
        "\n",
        "        # Index of dataframe has direct correspondence to item in dataset\n",
        "        self.df = self.df.assign(id=self.df.index.values)\n",
        "\n",
        "        # Convert arbitrary class names of dataset to ordered 0-(num_speakers - 1) integers\n",
        "        self.unique_characters = sorted(self.df['class_name'].unique())\n",
        "        self.class_name_to_id = {self.unique_characters[i]: i for i in range(self.num_classes())}\n",
        "        self.df = self.df.assign(class_id=self.df['class_name'].apply(lambda c: self.class_name_to_id[c]))\n",
        "\n",
        "        # Create dicts\n",
        "        self.datasetid_to_filepath = self.df.to_dict()['filepath']\n",
        "        self.datasetid_to_class_id = self.df.to_dict()['class_id']\n",
        "\n",
        "        # Setup transforms\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.Resize(84),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        instance = Image.open(self.datasetid_to_filepath[item])\n",
        "        instance = self.transform(instance)\n",
        "        label = self.datasetid_to_class_id[item]\n",
        "        return instance, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def num_classes(self):\n",
        "        return len(self.df['class_name'].unique())\n",
        "\n",
        "    @staticmethod\n",
        "    def index_subset(subset):\n",
        "        \"\"\"Index a subset by looping through all of its files and recording relevant information.\n",
        "        # Arguments\n",
        "            subset: Name of the subset\n",
        "        # Returns\n",
        "            A list of dicts containing information about all the image files in a particular subset of the\n",
        "            miniImageNet dataset\n",
        "        \"\"\"\n",
        "        images = []\n",
        "        print('Indexing {}...'.format(subset))\n",
        "        # Quick first pass to find total for tqdm bar\n",
        "        subset_len = 0\n",
        "        for root, folders, files in os.walk(DATA_PATH + '/miniImageNet/images_{}/'.format(subset)):\n",
        "            subset_len += len([f for f in files if f.endswith('.png')])\n",
        "\n",
        "        progress_bar = tqdm(total=subset_len)\n",
        "        for root, folders, files in os.walk(DATA_PATH + '/miniImageNet/images_{}/'.format(subset)):\n",
        "            if len(files) == 0:\n",
        "                continue\n",
        "\n",
        "            class_name = root.split('/')[-1]\n",
        "\n",
        "            for f in files:\n",
        "                progress_bar.update(1)\n",
        "                images.append({\n",
        "                    'subset': subset,\n",
        "                    'class_name': class_name,\n",
        "                    'filepath': os.path.join(root, f)\n",
        "                })\n",
        "\n",
        "        progress_bar.close()\n",
        "        return images\n",
        "\n",
        "\n",
        "class DummyDataset(Dataset):\n",
        "    def __init__(self, samples_per_class=10, n_classes=10, n_features=1):\n",
        "        \"\"\"Dummy dataset for debugging/testing purposes\n",
        "        A sample from the DummyDataset has (n_features + 1) features. The first feature is the index of the sample\n",
        "        in the data and the remaining features are the class index.\n",
        "        # Arguments\n",
        "            samples_per_class: Number of samples per class in the dataset\n",
        "            n_classes: Number of distinct classes in the dataset\n",
        "            n_features: Number of extra features each sample should have.\n",
        "        \"\"\"\n",
        "        self.samples_per_class = samples_per_class\n",
        "        self.n_classes = n_classes\n",
        "        self.n_features = n_features\n",
        "\n",
        "        # Create a dataframe to be consistent with other Datasets\n",
        "        self.df = pd.DataFrame({\n",
        "            'class_id': [i % self.n_classes for i in range(len(self))]\n",
        "        })\n",
        "        self.df = self.df.assign(id=self.df.index.values)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.samples_per_class * self.n_classes\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        class_id = item % self.n_classes\n",
        "        return np.array([item] + [class_id]*self.n_features, dtype=np.float), float(class_id)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing few_shot/datasets.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kv2HVP-kJcz",
        "outputId": "a9a205be-1119-4f34-f136-4632d76f41d2"
      },
      "source": [
        "# few_shot/metrics.py\n",
        "%%writefile few_shot/metrics.py\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "def categorical_accuracy(y, y_pred):\n",
        "    \"\"\"Calculates categorical accuracy.\n",
        "    # Arguments:\n",
        "        y_pred: Prediction probabilities or logits of shape [batch_size, num_categories]\n",
        "        y: Ground truth categories. Must have shape [batch_size,]\n",
        "    \"\"\"\n",
        "    return torch.eq(y_pred.argmax(dim=-1), y).sum().item() / y_pred.shape[0]\n",
        "\n",
        "\n",
        "NAMED_METRICS = {\n",
        "    'categorical_accuracy': categorical_accuracy\n",
        "}"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing few_shot/metrics.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ok1xXW_ikWBM",
        "outputId": "d3ff7461-bed8-4b08-fe85-8ffc07cf6525"
      },
      "source": [
        "# few_shot/eval.py\n",
        "%%writefile few_shot/eval.py\n",
        "\n",
        "import torch\n",
        "from torch.nn import Module\n",
        "from torch.utils.data import DataLoader\n",
        "from typing import Callable, List, Union\n",
        "\n",
        "from few_shot.metrics import NAMED_METRICS\n",
        "\n",
        "\n",
        "def evaluate(model: Module, dataloader: DataLoader, prepare_batch: Callable, metrics: List[Union[str, Callable]],\n",
        "             loss_fn: Callable = None, prefix: str = 'val_', suffix: str = ''):\n",
        "    \"\"\"Evaluate a model on one or more metrics on a particular dataset\n",
        "    # Arguments\n",
        "        model: Model to evaluate\n",
        "        dataloader: Instance of torch.utils.data.DataLoader representing the dataset\n",
        "        prepare_batch: Callable to perform any desired preprocessing\n",
        "        metrics: List of metrics to evaluate the model with. Metrics must either be a named metric (see `metrics.py`) or\n",
        "            a Callable that takes predictions and ground truth labels and returns a scalar value\n",
        "        loss_fn: Loss function to calculate over the dataset\n",
        "        prefix: Prefix to prepend to the name of each metric - used to identify the dataset. Defaults to 'val_' as\n",
        "            it is typical to evaluate on a held-out validation dataset\n",
        "        suffix: Suffix to append to the name of each metric.\n",
        "    \"\"\"\n",
        "    logs = {}\n",
        "    seen = 0\n",
        "    totals = {m: 0 for m in metrics}\n",
        "    if loss_fn is not None:\n",
        "        totals['loss'] = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            x, y = prepare_batch(batch)\n",
        "            y_pred = model(x)\n",
        "\n",
        "            seen += x.shape[0]\n",
        "\n",
        "            if loss_fn is not None:\n",
        "                totals['loss'] += loss_fn(y_pred, y).item() * x.shape[0]\n",
        "\n",
        "            for m in metrics:\n",
        "                if isinstance(m, str):\n",
        "                    v = NAMED_METRICS[m](y, y_pred)\n",
        "                else:\n",
        "                    # Assume metric is a callable function\n",
        "                    v = m(y, y_pred)\n",
        "\n",
        "                totals[m] += v * x.shape[0]\n",
        "\n",
        "    for m in ['loss'] + metrics:\n",
        "        logs[prefix + m + suffix] = totals[m] / seen\n",
        "\n",
        "    return logs"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing few_shot/eval.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akZbxV5lkPly",
        "outputId": "eb874876-4dce-484a-ef21-61353092fc0a"
      },
      "source": [
        "# few_shot/callbacks.py\n",
        "%%writefile few_shot/callbacks.py\n",
        "\"\"\"\n",
        "Ports of Callback classes from the Keras library.\n",
        "\"\"\"\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "from collections import OrderedDict, Iterable\n",
        "import warnings\n",
        "import os\n",
        "import csv\n",
        "import io\n",
        "\n",
        "from few_shot.eval import evaluate\n",
        "\n",
        "\n",
        "class CallbackList(object):\n",
        "    \"\"\"Container abstracting a list of callbacks.\n",
        "    # Arguments\n",
        "        callbacks: List of `Callback` instances.\n",
        "    \"\"\"\n",
        "    def __init__(self, callbacks):\n",
        "        self.callbacks = [c for c in callbacks]\n",
        "\n",
        "    def set_params(self, params):\n",
        "        for callback in self.callbacks:\n",
        "            callback.set_params(params)\n",
        "\n",
        "    def set_model(self, model):\n",
        "        for callback in self.callbacks:\n",
        "            callback.set_model(model)\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        \"\"\"Called at the start of an epoch.\n",
        "        # Arguments\n",
        "            epoch: integer, index of epoch.\n",
        "            logs: dictionary of logs.\n",
        "        \"\"\"\n",
        "        logs = logs or {}\n",
        "        for callback in self.callbacks:\n",
        "            callback.on_epoch_begin(epoch, logs)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        \"\"\"Called at the end of an epoch.\n",
        "        # Arguments\n",
        "            epoch: integer, index of epoch.\n",
        "            logs: dictionary of logs.\n",
        "        \"\"\"\n",
        "        logs = logs or {}\n",
        "        for callback in self.callbacks:\n",
        "            callback.on_epoch_end(epoch, logs)\n",
        "\n",
        "    def on_batch_begin(self, batch, logs=None):\n",
        "        \"\"\"Called right before processing a batch.\n",
        "        # Arguments\n",
        "            batch: integer, index of batch within the current epoch.\n",
        "            logs: dictionary of logs.\n",
        "        \"\"\"\n",
        "        logs = logs or {}\n",
        "        for callback in self.callbacks:\n",
        "            callback.on_batch_begin(batch, logs)\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        \"\"\"Called at the end of a batch.\n",
        "        # Arguments\n",
        "            batch: integer, index of batch within the current epoch.\n",
        "            logs: dictionary of logs.\n",
        "        \"\"\"\n",
        "        logs = logs or {}\n",
        "        for callback in self.callbacks:\n",
        "            callback.on_batch_end(batch, logs)\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        \"\"\"Called at the beginning of training.\n",
        "        # Arguments\n",
        "            logs: dictionary of logs.\n",
        "        \"\"\"\n",
        "        logs = logs or {}\n",
        "        for callback in self.callbacks:\n",
        "            callback.on_train_begin(logs)\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        \"\"\"Called at the end of training.\n",
        "        # Arguments\n",
        "            logs: dictionary of logs.\n",
        "        \"\"\"\n",
        "        logs = logs or {}\n",
        "        for callback in self.callbacks:\n",
        "            callback.on_train_end(logs)\n",
        "\n",
        "\n",
        "class Callback(object):\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "\n",
        "    def set_params(self, params):\n",
        "        self.params = params\n",
        "\n",
        "    def set_model(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        pass\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        pass\n",
        "\n",
        "    def on_batch_begin(self, batch, logs=None):\n",
        "        pass\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        pass\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        pass\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        pass\n",
        "\n",
        "\n",
        "class DefaultCallback(Callback):\n",
        "    \"\"\"Records metrics over epochs by averaging over each batch.\n",
        "    NB The metrics are calculated with a moving model\n",
        "    \"\"\"\n",
        "    def on_epoch_begin(self, batch, logs=None):\n",
        "        self.seen = 0\n",
        "        self.totals = {}\n",
        "        self.metrics = ['loss'] + self.params['metrics']\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        logs = logs or {}\n",
        "        batch_size = logs.get('size', 1) or 1\n",
        "        self.seen += batch_size\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            if k in self.totals:\n",
        "                self.totals[k] += v * batch_size\n",
        "            else:\n",
        "                self.totals[k] = v * batch_size\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs is not None:\n",
        "            for k in self.metrics:\n",
        "                if k in self.totals:\n",
        "                    # Make value available to next callbacks.\n",
        "                    logs[k] = self.totals[k] / self.seen\n",
        "\n",
        "\n",
        "class ProgressBarLogger(Callback):\n",
        "    \"\"\"TQDM progress bar that displays the running average of loss and other metrics.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(ProgressBarLogger, self).__init__()\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.num_batches = self.params['num_batches']\n",
        "        self.verbose = self.params['verbose']\n",
        "        self.metrics = ['loss'] + self.params['metrics']\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        self.target = self.num_batches\n",
        "        self.pbar = tqdm(total=self.target, desc='Epoch {}'.format(epoch))\n",
        "        self.seen = 0\n",
        "\n",
        "    def on_batch_begin(self, batch, logs=None):\n",
        "        self.log_values = {}\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        logs = logs or {}\n",
        "        self.seen += 1\n",
        "\n",
        "        for k in self.metrics:\n",
        "            if k in logs:\n",
        "                self.log_values[k] = logs[k]\n",
        "\n",
        "        # Skip progbar update for the last batch;\n",
        "        # will be handled by on_epoch_end.\n",
        "        if self.verbose and self.seen < self.target:\n",
        "            self.pbar.update(1)\n",
        "            self.pbar.set_postfix(self.log_values)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Update log values\n",
        "        self.log_values = {}\n",
        "        for k in self.metrics:\n",
        "            if k in logs:\n",
        "                self.log_values[k] = logs[k]\n",
        "\n",
        "        if self.verbose:\n",
        "            self.pbar.update(1)\n",
        "            self.pbar.set_postfix(self.log_values)\n",
        "\n",
        "        self.pbar.close()\n",
        "\n",
        "\n",
        "class CSVLogger(Callback):\n",
        "    \"\"\"Callback that streams epoch results to a csv file.\n",
        "    Supports all values that can be represented as a string,\n",
        "    including 1D iterables such as np.ndarray.\n",
        "    # Arguments\n",
        "        filename: filename of the csv file, e.g. 'run/log.csv'.\n",
        "        separator: string used to separate elements in the csv file.\n",
        "        append: True: append if file exists (useful for continuing\n",
        "            training). False: overwrite existing file,\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, filename, separator=',', append=False):\n",
        "        self.sep = separator\n",
        "        self.filename = filename\n",
        "        self.append = append\n",
        "        self.writer = None\n",
        "        self.keys = None\n",
        "        self.append_header = True\n",
        "        self.file_flags = ''\n",
        "        self._open_args = {'newline': '\\n'}\n",
        "        super(CSVLogger, self).__init__()\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        if self.append:\n",
        "            if os.path.exists(self.filename):\n",
        "                with open(self.filename, 'r' + self.file_flags) as f:\n",
        "                    self.append_header = not bool(len(f.readline()))\n",
        "            mode = 'a'\n",
        "        else:\n",
        "            mode = 'w'\n",
        "\n",
        "        self.csv_file = io.open(self.filename,\n",
        "                                mode + self.file_flags,\n",
        "                                **self._open_args)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "\n",
        "        def handle_value(k):\n",
        "            is_zero_dim_ndarray = isinstance(k, np.ndarray) and k.ndim == 0\n",
        "            if isinstance(k, str):\n",
        "                return k\n",
        "            elif isinstance(k, Iterable) and not is_zero_dim_ndarray:\n",
        "                return '\"[%s]\"' % (', '.join(map(str, k)))\n",
        "            else:\n",
        "                return k\n",
        "\n",
        "        if self.keys is None:\n",
        "            self.keys = sorted(logs.keys())\n",
        "\n",
        "        if not self.writer:\n",
        "            class CustomDialect(csv.excel):\n",
        "                delimiter = self.sep\n",
        "            fieldnames = ['epoch'] + self.keys\n",
        "            self.writer = csv.DictWriter(self.csv_file,\n",
        "                                         fieldnames=fieldnames,\n",
        "                                         dialect=CustomDialect)\n",
        "            if self.append_header:\n",
        "                self.writer.writeheader()\n",
        "\n",
        "        row_dict = OrderedDict({'epoch': epoch})\n",
        "        row_dict.update((key, handle_value(logs[key])) for key in self.keys)\n",
        "        self.writer.writerow(row_dict)\n",
        "        self.csv_file.flush()\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        self.csv_file.close()\n",
        "        self.writer = None\n",
        "\n",
        "\n",
        "class EvaluateMetrics(Callback):\n",
        "    \"\"\"Evaluates metrics on a dataset after every epoch.\n",
        "    # Argments\n",
        "        dataloader: torch.DataLoader of the dataset on which the model will be evaluated\n",
        "        prefix: Prefix to prepend to the names of the metrics when they is logged. Defaults to 'val_' but can be changed\n",
        "        if the model is to be evaluated on many datasets separately.\n",
        "        suffix: Suffix to append to the names of the metrics when they is logged.\n",
        "    \"\"\"\n",
        "    def __init__(self, dataloader, prefix='val_', suffix=''):\n",
        "        super(EvaluateMetrics, self).__init__()\n",
        "        self.dataloader = dataloader\n",
        "        self.prefix = prefix\n",
        "        self.suffix = suffix\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.metrics = self.params['metrics']\n",
        "        self.prepare_batch = self.params['prepare_batch']\n",
        "        self.loss_fn = self.params['loss_fn']\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        logs.update(\n",
        "            evaluate(self.model, self.dataloader, self.prepare_batch, self.metrics, self.loss_fn, self.prefix, self.suffix)\n",
        "        )\n",
        "\n",
        "\n",
        "class ReduceLROnPlateau(Callback):\n",
        "    \"\"\"Reduce learning rate when a metric has stopped improving.\n",
        "    Models often benefit from reducing the learning rate by a factor\n",
        "    of 2-10 once learning stagnates. This callback monitors a\n",
        "    quantity and if no improvement is seen for a 'patience' number\n",
        "    of epochs, the learning rate is reduced.\n",
        "    # Arguments\n",
        "        monitor: quantity to be monitored.\n",
        "        factor: factor by which the learning rate will\n",
        "            be reduced. new_lr = lr * factor\n",
        "        patience: number of epochs with no improvement\n",
        "            after which learning rate will be reduced.\n",
        "        verbose: int. 0: quiet, 1: update messages.\n",
        "        mode: one of {auto, min, max}. In `min` mode,\n",
        "            lr will be reduced when the quantity\n",
        "            monitored has stopped decreasing; in `max`\n",
        "            mode it will be reduced when the quantity\n",
        "            monitored has stopped increasing; in `auto`\n",
        "            mode, the direction is automatically inferred\n",
        "            from the name of the monitored quantity.\n",
        "        min_delta: threshold for measuring the new optimum,\n",
        "            to only focus on significant changes.\n",
        "        cooldown: number of epochs to wait before resuming\n",
        "            normal operation after lr has been reduced.\n",
        "        min_lr: lower bound on the learning rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, monitor='val_loss', factor=0.1, patience=10,\n",
        "                 verbose=0, mode='auto', min_delta=1e-4, cooldown=0, min_lr=0,\n",
        "                 **kwargs):\n",
        "        super(ReduceLROnPlateau, self).__init__()\n",
        "\n",
        "        self.monitor = monitor\n",
        "        if factor >= 1.0:\n",
        "            raise ValueError('ReduceLROnPlateau does not support a factor >= 1.0.')\n",
        "        self.factor = factor\n",
        "        self.min_lr = min_lr\n",
        "        self.min_delta = min_delta\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.cooldown = cooldown\n",
        "        self.cooldown_counter = 0  # Cooldown counter.\n",
        "        self.wait = 0\n",
        "        self.best = 0\n",
        "        if mode not in ['auto', 'min', 'max']:\n",
        "            raise ValueError('Mode must be one of (auto, min, max).')\n",
        "        self.mode = mode\n",
        "        self.monitor_op = None\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "        \"\"\"Resets wait counter and cooldown counter.\n",
        "        \"\"\"\n",
        "        if (self.mode == 'min' or\n",
        "                (self.mode == 'auto' and 'acc' not in self.monitor)):\n",
        "            self.monitor_op = lambda a, b: np.less(a, b - self.min_delta)\n",
        "            self.best = np.Inf\n",
        "        else:\n",
        "            self.monitor_op = lambda a, b: np.greater(a, b + self.min_delta)\n",
        "            self.best = -np.Inf\n",
        "        self.cooldown_counter = 0\n",
        "        self.wait = 0\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.optimiser = self.params['optimiser']\n",
        "        self.min_lrs = [self.min_lr] * len(self.optimiser.param_groups)\n",
        "        self._reset()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        if len(self.optimiser.param_groups) == 1:\n",
        "            logs['lr'] = self.optimiser.param_groups[0]['lr']\n",
        "        else:\n",
        "            for i, param_group in enumerate(self.optimiser.param_groups):\n",
        "                logs['lr_{}'.format(i)] = param_group['lr']\n",
        "\n",
        "        current = logs.get(self.monitor)\n",
        "\n",
        "        if self.in_cooldown():\n",
        "            self.cooldown_counter -= 1\n",
        "            self.wait = 0\n",
        "\n",
        "        if self.monitor_op(current, self.best):\n",
        "            self.best = current\n",
        "            self.wait = 0\n",
        "        elif not self.in_cooldown():\n",
        "            self.wait += 1\n",
        "            if self.wait >= self.patience:\n",
        "                self._reduce_lr(epoch)\n",
        "                self.cooldown_counter = self.cooldown\n",
        "                self.wait = 0\n",
        "\n",
        "    def _reduce_lr(self, epoch):\n",
        "        for i, param_group in enumerate(self.optimiser.param_groups):\n",
        "            old_lr = float(param_group['lr'])\n",
        "            new_lr = max(old_lr * self.factor, self.min_lrs[i])\n",
        "            if old_lr - new_lr > self.min_delta:\n",
        "                param_group['lr'] = new_lr\n",
        "                if self.verbose:\n",
        "                    print('Epoch {:5d}: reducing learning rate'\n",
        "                          ' of group {} to {:.4e}.'.format(epoch, i, new_lr))\n",
        "\n",
        "    def in_cooldown(self):\n",
        "        return self.cooldown_counter > 0\n",
        "\n",
        "\n",
        "class ModelCheckpoint(Callback):\n",
        "    \"\"\"Save the model after every epoch.\n",
        "    `filepath` can contain named formatting options, which will be filled the value of `epoch` and keys in `logs`\n",
        "    (passed in `on_epoch_end`).\n",
        "    For example: if `filepath` is `weights.{epoch:02d}-{val_loss:.2f}.hdf5`, then the model checkpoints will be saved\n",
        "    with the epoch number and the validation loss in the filename.\n",
        "    # Arguments\n",
        "        filepath: string, path to save the model file.\n",
        "        monitor: quantity to monitor.\n",
        "        verbose: verbosity mode, 0 or 1.\n",
        "        save_best_only: if `save_best_only=True`,\n",
        "            the latest best model according to\n",
        "            the quantity monitored will not be overwritten.\n",
        "        mode: one of {auto, min, max}.\n",
        "            If `save_best_only=True`, the decision\n",
        "            to overwrite the current save file is made\n",
        "            based on either the maximization or the\n",
        "            minimization of the monitored quantity. For `val_acc`,\n",
        "            this should be `max`, for `val_loss` this should\n",
        "            be `min`, etc. In `auto` mode, the direction is\n",
        "            automatically inferred from the name of the monitored quantity.\n",
        "        save_weights_only: if True, then only the model's weights will be\n",
        "            saved (`model.save_weights(filepath)`), else the full model\n",
        "            is saved (`model.save(filepath)`).\n",
        "        period: Interval (number of epochs) between checkpoints.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, filepath, monitor='val_loss', verbose=0, save_best_only=False, mode='auto', period=1):\n",
        "        super(ModelCheckpoint, self).__init__()\n",
        "        self.monitor = monitor\n",
        "        self.verbose = verbose\n",
        "        self.filepath = filepath\n",
        "        self.save_best_only = save_best_only\n",
        "        self.period = period\n",
        "        self.epochs_since_last_save = 0\n",
        "\n",
        "        if mode not in ['auto', 'min', 'max']:\n",
        "            raise ValueError('Mode must be one of (auto, min, max).')\n",
        "\n",
        "        if mode == 'min':\n",
        "            self.monitor_op = np.less\n",
        "            self.best = np.Inf\n",
        "        elif mode == 'max':\n",
        "            self.monitor_op = np.greater\n",
        "            self.best = -np.Inf\n",
        "        else:\n",
        "            if 'acc' in self.monitor or self.monitor.startswith('fmeasure'):\n",
        "                self.monitor_op = np.greater\n",
        "                self.best = -np.Inf\n",
        "            else:\n",
        "                self.monitor_op = np.less\n",
        "\n",
        "        self.best = np.Inf\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        self.epochs_since_last_save += 1\n",
        "        if self.epochs_since_last_save >= self.period:\n",
        "            self.epochs_since_last_save = 0\n",
        "            filepath = self.filepath.format(epoch=epoch + 1, **logs)\n",
        "            if self.save_best_only:\n",
        "                current = logs.get(self.monitor)\n",
        "                if current is None:\n",
        "                    warnings.warn('Can save best model only with %s available, '\n",
        "                                  'skipping.' % (self.monitor), RuntimeWarning)\n",
        "                else:\n",
        "                    if self.monitor_op(current, self.best):\n",
        "                        if self.verbose > 0:\n",
        "                            print('\\nEpoch %05d: %s improved from %0.5f to %0.5f,'\n",
        "                                  ' saving model to %s'\n",
        "                                  % (epoch + 1, self.monitor, self.best,\n",
        "                                     current, filepath))\n",
        "                        self.best = current\n",
        "                        torch.save(self.model.state_dict(), filepath)\n",
        "                    else:\n",
        "                        if self.verbose > 0:\n",
        "                            print('\\nEpoch %05d: %s did not improve from %0.5f' %\n",
        "                                  (epoch + 1, self.monitor, self.best))\n",
        "            else:\n",
        "                if self.verbose > 0:\n",
        "                    print('\\nEpoch %05d: saving model to %s' % (epoch + 1, filepath))\n",
        "                torch.save(self.model.state_dict(), filepath)\n",
        "\n",
        "\n",
        "class LearningRateScheduler(Callback):\n",
        "    \"\"\"Learning rate scheduler.\n",
        "    # Arguments\n",
        "        schedule: a function that takes an epoch index as input\n",
        "            (integer, indexed from 0) and current learning rate\n",
        "            and returns a new learning rate as output (float).\n",
        "        verbose: int. 0: quiet, 1: update messages.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, schedule, verbose=0):\n",
        "        super(LearningRateScheduler, self).__init__()\n",
        "        self.schedule = schedule\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.optimiser = self.params['optimiser']\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        lrs = [self.schedule(epoch, param_group['lr']) for param_group in self.optimiser.param_groups]\n",
        "\n",
        "        if not all(isinstance(lr, (float, np.float32, np.float64)) for lr in lrs):\n",
        "            raise ValueError('The output of the \"schedule\" function '\n",
        "                             'should be float.')\n",
        "        self.set_lr(epoch, lrs)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        if len(self.optimiser.param_groups) == 1:\n",
        "            logs['lr'] = self.optimiser.param_groups[0]['lr']\n",
        "        else:\n",
        "            for i, param_group in enumerate(self.optimiser.param_groups):\n",
        "                logs['lr_{}'.format(i)] = param_group['lr']\n",
        "\n",
        "    def set_lr(self, epoch, lrs):\n",
        "        for i, param_group in enumerate(self.optimiser.param_groups):\n",
        "            new_lr = lrs[i]\n",
        "            param_group['lr'] = new_lr\n",
        "            if self.verbose:\n",
        "                print('Epoch {:5d}: setting learning rate'\n",
        "                      ' of group {} to {:.4e}.'.format(epoch, i, new_lr))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing few_shot/callbacks.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MWtmlxNkD32",
        "outputId": "d8c1b677-3936-46b9-f321-3b32c40bd8e3"
      },
      "source": [
        "# few_shot/core.py\n",
        "%%writefile few_shot/core.py\n",
        "\n",
        "from torch.utils.data import Sampler\n",
        "from typing import List, Iterable, Callable, Tuple\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from few_shot.metrics import categorical_accuracy\n",
        "from few_shot.callbacks import Callback\n",
        "\n",
        "\n",
        "class NShotTaskSampler(Sampler):\n",
        "    def __init__(self,\n",
        "                 dataset: torch.utils.data.Dataset,\n",
        "                 episodes_per_epoch: int = None,\n",
        "                 n: int = None,\n",
        "                 k: int = None,\n",
        "                 q: int = None,\n",
        "                 num_tasks: int = 1,\n",
        "                 fixed_tasks: List[Iterable[int]] = None):\n",
        "        \"\"\"PyTorch Sampler subclass that generates batches of n-shot, k-way, q-query tasks.\n",
        "        Each n-shot task contains a \"support set\" of `k` sets of `n` samples and a \"query set\" of `k` sets\n",
        "        of `q` samples. The support set and the query set are all grouped into one Tensor such that the first n * k\n",
        "        samples are from the support set while the remaining q * k samples are from the query set.\n",
        "        The support and query sets are sampled such that they are disjoint i.e. do not contain overlapping samples.\n",
        "        # Arguments\n",
        "            dataset: Instance of torch.utils.data.Dataset from which to draw samples\n",
        "            episodes_per_epoch: Arbitrary number of batches of n-shot tasks to generate in one epoch\n",
        "            n_shot: int. Number of samples for each class in the n-shot classification tasks.\n",
        "            k_way: int. Number of classes in the n-shot classification tasks.\n",
        "            q_queries: int. Number query samples for each class in the n-shot classification tasks.\n",
        "            num_tasks: Number of n-shot tasks to group into a single batch\n",
        "            fixed_tasks: If this argument is specified this Sampler will always generate tasks from\n",
        "                the specified classes\n",
        "        \"\"\"\n",
        "        super(NShotTaskSampler, self).__init__(dataset)\n",
        "        self.episodes_per_epoch = episodes_per_epoch\n",
        "        self.dataset = dataset\n",
        "        if num_tasks < 1:\n",
        "            raise ValueError('num_tasks must be > 1.')\n",
        "\n",
        "        self.num_tasks = num_tasks\n",
        "        # TODO: Raise errors if initialise badly\n",
        "        self.k = k\n",
        "        self.n = n\n",
        "        self.q = q\n",
        "        self.fixed_tasks = fixed_tasks\n",
        "\n",
        "        self.i_task = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.episodes_per_epoch\n",
        "\n",
        "    def __iter__(self):\n",
        "        for _ in range(self.episodes_per_epoch):\n",
        "            batch = []\n",
        "\n",
        "            for task in range(self.num_tasks):\n",
        "                if self.fixed_tasks is None:\n",
        "                    # Get random classes\n",
        "                    episode_classes = np.random.choice(self.dataset.df['class_id'].unique(), size=self.k, replace=False)\n",
        "                else:\n",
        "                    # Loop through classes in fixed_tasks\n",
        "                    episode_classes = self.fixed_tasks[self.i_task % len(self.fixed_tasks)]\n",
        "                    self.i_task += 1\n",
        "\n",
        "                df = self.dataset.df[self.dataset.df['class_id'].isin(episode_classes)]\n",
        "\n",
        "                support_k = {k: None for k in episode_classes}\n",
        "                for k in episode_classes:\n",
        "                    # Select support examples\n",
        "                    support = df[df['class_id'] == k].sample(self.n)\n",
        "                    support_k[k] = support\n",
        "\n",
        "                    for i, s in support.iterrows():\n",
        "                        batch.append(s['id'])\n",
        "\n",
        "                for k in episode_classes:\n",
        "                    query = df[(df['class_id'] == k) & (~df['id'].isin(support_k[k]['id']))].sample(self.q)\n",
        "                    for i, q in query.iterrows():\n",
        "                        batch.append(q['id'])\n",
        "\n",
        "            yield np.stack(batch)\n",
        "\n",
        "\n",
        "class EvaluateFewShot(Callback):\n",
        "    \"\"\"Evaluate a network on  an n-shot, k-way classification tasks after every epoch.\n",
        "    # Arguments\n",
        "        eval_fn: Callable to perform few-shot classification. Examples include `proto_net_episode`,\n",
        "            `matching_net_episode` and `meta_gradient_step` (MAML).\n",
        "        num_tasks: int. Number of n-shot classification tasks to evaluate the model with.\n",
        "        n_shot: int. Number of samples for each class in the n-shot classification tasks.\n",
        "        k_way: int. Number of classes in the n-shot classification tasks.\n",
        "        q_queries: int. Number query samples for each class in the n-shot classification tasks.\n",
        "        task_loader: Instance of NShotWrapper class\n",
        "        prepare_batch: function. The preprocessing function to apply to samples from the dataset.\n",
        "        prefix: str. Prefix to identify dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 eval_fn: Callable,\n",
        "                 num_tasks: int,\n",
        "                 n_shot: int,\n",
        "                 k_way: int,\n",
        "                 q_queries: int,\n",
        "                 taskloader: torch.utils.data.DataLoader,\n",
        "                 prepare_batch: Callable,\n",
        "                 prefix: str = 'val_',\n",
        "                 **kwargs):\n",
        "        super(EvaluateFewShot, self).__init__()\n",
        "        self.eval_fn = eval_fn\n",
        "        self.num_tasks = num_tasks\n",
        "        self.n_shot = n_shot\n",
        "        self.k_way = k_way\n",
        "        self.q_queries = q_queries\n",
        "        self.taskloader = taskloader\n",
        "        self.prepare_batch = prepare_batch\n",
        "        self.prefix = prefix\n",
        "        self.kwargs = kwargs\n",
        "        self.metric_name = f'{self.prefix}{self.n_shot}-shot_{self.k_way}-way_acc'\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.loss_fn = self.params['loss_fn']\n",
        "        self.optimiser = self.params['optimiser']\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        seen = 0\n",
        "        totals = {'loss': 0, self.metric_name: 0}\n",
        "        for batch_index, batch in enumerate(self.taskloader):\n",
        "            x, y = self.prepare_batch(batch)\n",
        "\n",
        "            loss, y_pred = self.eval_fn(\n",
        "                self.model,\n",
        "                self.optimiser,\n",
        "                self.loss_fn,\n",
        "                x,\n",
        "                y,\n",
        "                n_shot=self.n_shot,\n",
        "                k_way=self.k_way,\n",
        "                q_queries=self.q_queries,\n",
        "                train=False,\n",
        "                **self.kwargs\n",
        "            )\n",
        "\n",
        "            seen += y_pred.shape[0]\n",
        "\n",
        "            totals['loss'] += loss.item() * y_pred.shape[0]\n",
        "            totals[self.metric_name] += categorical_accuracy(y, y_pred) * y_pred.shape[0]\n",
        "\n",
        "        logs[self.prefix + 'loss'] = totals['loss'] / seen\n",
        "        logs[self.metric_name] = totals[self.metric_name] / seen\n",
        "\n",
        "\n",
        "def prepare_nshot_task(n: int, k: int, q: int) -> Callable:\n",
        "    \"\"\"Typical n-shot task preprocessing.\n",
        "    # Arguments\n",
        "        n: Number of samples for each class in the n-shot classification task\n",
        "        k: Number of classes in the n-shot classification task\n",
        "        q: Number of query samples for each class in the n-shot classification task\n",
        "    # Returns\n",
        "        prepare_nshot_task_: A Callable that processes a few shot tasks with specified n, k and q\n",
        "    \"\"\"\n",
        "    def prepare_nshot_task_(batch: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Create 0-k label and move to GPU.\n",
        "        TODO: Move to arbitrary device\n",
        "        \"\"\"\n",
        "        x, y = batch\n",
        "        x = x.double().cuda()\n",
        "        # Create dummy 0-(num_classes - 1) label\n",
        "        y = create_nshot_task_label(k, q).cuda()\n",
        "        return x, y\n",
        "\n",
        "    return prepare_nshot_task_\n",
        "\n",
        "\n",
        "def create_nshot_task_label(k: int, q: int) -> torch.Tensor:\n",
        "    \"\"\"Creates an n-shot task label.\n",
        "    Label has the structure:\n",
        "        [0]*q + [1]*q + ... + [k-1]*q\n",
        "    # TODO: Test this\n",
        "    # Arguments\n",
        "        k: Number of classes in the n-shot classification task\n",
        "        q: Number of query samples for each class in the n-shot classification task\n",
        "    # Returns\n",
        "        y: Label vector for n-shot task of shape [q * k, ]\n",
        "    \"\"\"\n",
        "    y = torch.arange(0, k, 1 / q).long()\n",
        "    return y"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing few_shot/core.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_ljCtFElXfp",
        "outputId": "58b0a2bc-6d0f-44cf-de35-30e39bd87b41"
      },
      "source": [
        "# few_shot/maml.py\n",
        "%%writefile few_shot/maml.py\n",
        "\n",
        "import torch\n",
        "from collections import OrderedDict\n",
        "from torch.optim import Optimizer\n",
        "from torch.nn import Module\n",
        "from typing import Dict, List, Callable, Union\n",
        "\n",
        "from few_shot.core import create_nshot_task_label\n",
        "\n",
        "\n",
        "def replace_grad(parameter_gradients, parameter_name):\n",
        "    def replace_grad_(module):\n",
        "        return parameter_gradients[parameter_name]\n",
        "\n",
        "    return replace_grad_\n",
        "\n",
        "\n",
        "def meta_gradient_step(model: Module,\n",
        "                       optimiser: Optimizer,\n",
        "                       loss_fn: Callable,\n",
        "                       x: torch.Tensor,\n",
        "                       y: torch.Tensor,\n",
        "                       n_shot: int,\n",
        "                       k_way: int,\n",
        "                       q_queries: int,\n",
        "                       order: int,\n",
        "                       inner_train_steps: int,\n",
        "                       inner_lr: float,\n",
        "                       train: bool,\n",
        "                       device: Union[str, torch.device]):\n",
        "    \"\"\"\n",
        "    Perform a gradient step on a meta-learner.\n",
        "    # Arguments\n",
        "        model: Base model of the meta-learner being trained\n",
        "        optimiser: Optimiser to calculate gradient step from loss\n",
        "        loss_fn: Loss function to calculate between predictions and outputs\n",
        "        x: Input samples for all few shot tasks\n",
        "        y: Input labels of all few shot tasks\n",
        "        n_shot: Number of examples per class in the support set of each task\n",
        "        k_way: Number of classes in the few shot classification task of each task\n",
        "        q_queries: Number of examples per class in the query set of each task. The query set is used to calculate\n",
        "            meta-gradients after applying the update to\n",
        "        order: Whether to use 1st order MAML (update meta-learner weights with gradients of the updated weights on the\n",
        "            query set) or 2nd order MAML (use 2nd order updates by differentiating through the gradients of the updated\n",
        "            weights on the query with respect to the original weights).\n",
        "        inner_train_steps: Number of gradient steps to fit the fast weights during each inner update\n",
        "        inner_lr: Learning rate used to update the fast weights on the inner update\n",
        "        train: Whether to update the meta-learner weights at the end of the episode.\n",
        "        device: Device on which to run computation\n",
        "    \"\"\"\n",
        "    data_shape = x.shape[2:]\n",
        "    create_graph = (True if order == 2 else False) and train\n",
        "\n",
        "    task_gradients = []\n",
        "    task_losses = []\n",
        "    task_predictions = []\n",
        "    for meta_batch in x:\n",
        "        # By construction x is a 5D tensor of shape: (meta_batch_size, n*k + q*k, channels, width, height)\n",
        "        # Hence when we iterate over the first  dimension we are iterating through the meta batches\n",
        "        x_task_train = meta_batch[:n_shot * k_way]\n",
        "        x_task_val = meta_batch[n_shot * k_way:]\n",
        "\n",
        "        # Create a fast model using the current meta model weights\n",
        "        fast_weights = OrderedDict(model.named_parameters())\n",
        "\n",
        "        # Train the model for `inner_train_steps` iterations\n",
        "        for inner_batch in range(inner_train_steps):\n",
        "            # Perform update of model weights\n",
        "            y = create_nshot_task_label(k_way, n_shot).to(device)\n",
        "            logits = model.functional_forward(x_task_train, fast_weights)\n",
        "            loss = loss_fn(logits, y)\n",
        "            gradients = torch.autograd.grad(loss, fast_weights.values(), create_graph=create_graph)\n",
        "\n",
        "            # Update weights manually\n",
        "            fast_weights = OrderedDict(\n",
        "                (name, param - inner_lr * grad)\n",
        "                for ((name, param), grad) in zip(fast_weights.items(), gradients)\n",
        "            )\n",
        "\n",
        "        # Do a pass of the model on the validation data from the current task\n",
        "        y = create_nshot_task_label(k_way, q_queries).to(device)\n",
        "        logits = model.functional_forward(x_task_val, fast_weights)\n",
        "        loss = loss_fn(logits, y)\n",
        "        loss.backward(retain_graph=True)\n",
        "\n",
        "        # Get post-update accuracies\n",
        "        y_pred = logits.softmax(dim=1)\n",
        "        task_predictions.append(y_pred)\n",
        "\n",
        "        # Accumulate losses and gradients\n",
        "        task_losses.append(loss)\n",
        "        gradients = torch.autograd.grad(loss, fast_weights.values(), create_graph=create_graph)\n",
        "        named_grads = {name: g for ((name, _), g) in zip(fast_weights.items(), gradients)}\n",
        "        task_gradients.append(named_grads)\n",
        "\n",
        "    if order == 1:\n",
        "        if train:\n",
        "            sum_task_gradients = {k: torch.stack([grad[k] for grad in task_gradients]).mean(dim=0)\n",
        "                                  for k in task_gradients[0].keys()}\n",
        "            hooks = []\n",
        "            for name, param in model.named_parameters():\n",
        "                hooks.append(\n",
        "                    param.register_hook(replace_grad(sum_task_gradients, name))\n",
        "                )\n",
        "\n",
        "            model.train()\n",
        "            optimiser.zero_grad()\n",
        "            # Dummy pass in order to create `loss` variable\n",
        "            # Replace dummy gradients with mean task gradients using hooks\n",
        "            logits = model(torch.zeros((k_way, ) + data_shape).to(device, dtype=torch.double))\n",
        "            loss = loss_fn(logits, create_nshot_task_label(k_way, 1).to(device))\n",
        "            loss.backward()\n",
        "            optimiser.step()\n",
        "\n",
        "            for h in hooks:\n",
        "                h.remove()\n",
        "\n",
        "        return torch.stack(task_losses).mean(), torch.cat(task_predictions)\n",
        "\n",
        "    elif order == 2:\n",
        "        model.train()\n",
        "        optimiser.zero_grad()\n",
        "        meta_batch_loss = torch.stack(task_losses).mean()\n",
        "\n",
        "        if train:\n",
        "            meta_batch_loss.backward()\n",
        "            optimiser.step()\n",
        "\n",
        "        return meta_batch_loss, torch.cat(task_predictions)\n",
        "    else:\n",
        "        raise ValueError('Order must be either 1 or 2.')\n",
        "\n",
        "def replace_grad(parameter_gradients, parameter_name):\n",
        "    def replace_grad_(module):\n",
        "        return parameter_gradients[parameter_name]\n",
        "\n",
        "    return replace_grad_\n",
        "\n",
        "\n",
        "def meta_gradient_step(model: Module,\n",
        "                       optimiser: Optimizer,\n",
        "                       loss_fn: Callable,\n",
        "                       x: torch.Tensor,\n",
        "                       y: torch.Tensor,\n",
        "                       n_shot: int,\n",
        "                       k_way: int,\n",
        "                       q_queries: int,\n",
        "                       order: int,\n",
        "                       inner_train_steps: int,\n",
        "                       inner_lr: float,\n",
        "                       train: bool,\n",
        "                       device: Union[str, torch.device]):\n",
        "    \"\"\"\n",
        "    Perform a gradient step on a meta-learner.\n",
        "    # Arguments\n",
        "        model: Base model of the meta-learner being trained\n",
        "        optimiser: Optimiser to calculate gradient step from loss\n",
        "        loss_fn: Loss function to calculate between predictions and outputs\n",
        "        x: Input samples for all few shot tasks\n",
        "        y: Input labels of all few shot tasks\n",
        "        n_shot: Number of examples per class in the support set of each task\n",
        "        k_way: Number of classes in the few shot classification task of each task\n",
        "        q_queries: Number of examples per class in the query set of each task. The query set is used to calculate\n",
        "            meta-gradients after applying the update to\n",
        "        order: Whether to use 1st order MAML (update meta-learner weights with gradients of the updated weights on the\n",
        "            query set) or 2nd order MAML (use 2nd order updates by differentiating through the gradients of the updated\n",
        "            weights on the query with respect to the original weights).\n",
        "        inner_train_steps: Number of gradient steps to fit the fast weights during each inner update\n",
        "        inner_lr: Learning rate used to update the fast weights on the inner update\n",
        "        train: Whether to update the meta-learner weights at the end of the episode.\n",
        "        device: Device on which to run computation\n",
        "    \"\"\"\n",
        "    data_shape = x.shape[2:]\n",
        "    create_graph = (True if order == 2 else False) and train\n",
        "\n",
        "    task_gradients = []\n",
        "    task_losses = []\n",
        "    task_predictions = []\n",
        "    for meta_batch in x:\n",
        "        # By construction x is a 5D tensor of shape: (meta_batch_size, n*k + q*k, channels, width, height)\n",
        "        # Hence when we iterate over the first  dimension we are iterating through the meta batches\n",
        "        x_task_train = meta_batch[:n_shot * k_way]\n",
        "        x_task_val = meta_batch[n_shot * k_way:]\n",
        "\n",
        "        # Create a fast model using the current meta model weights\n",
        "        # For each n-shot task in a meta-batch of tasks, create a new model using the weights of the base model AKA meta-learner\n",
        "        fast_weights = OrderedDict(model.named_parameters()) \n",
        "\n",
        "        # Train the model for `inner_train_steps` iterations\n",
        "        # Update the weights of the new model using the loss from the samples in the task by stochastic gradient descent\n",
        "        for inner_batch in range(inner_train_steps):\n",
        "            # Perform update of model weights\n",
        "            y = create_nshot_task_label(k_way, n_shot).to(device)\n",
        "            logits = model.functional_forward(x_task_train, fast_weights)\n",
        "            loss = loss_fn(logits, y)\n",
        "            gradients = torch.autograd.grad(loss, fast_weights.values(), create_graph=create_graph)\n",
        "\n",
        "            # Update weights manually\n",
        "            fast_weights = OrderedDict(\n",
        "                (name, param - inner_lr * grad)\n",
        "                for ((name, param), grad) in zip(fast_weights.items(), gradients)\n",
        "            )\n",
        "\n",
        "        # Do a pass of the model on the validation data from the current task\n",
        "        # Calculate loss of the updated model on some more data from the same task \n",
        "        y = create_nshot_task_label(k_way, q_queries).to(device)\n",
        "        logits = model.functional_forward(x_task_val, fast_weights)\n",
        "        loss = loss_fn(logits, y)\n",
        "        loss.backward(retain_graph=True)\n",
        "\n",
        "        # Get post-update accuracies\n",
        "        y_pred = logits.softmax(dim=1)\n",
        "        task_predictions.append(y_pred)\n",
        "\n",
        "        # Accumulate losses and gradients\n",
        "        task_losses.append(loss)\n",
        "        gradients = torch.autograd.grad(loss, fast_weights.values(), create_graph=create_graph)\n",
        "        named_grads = {name: g for ((name, _), g) in zip(fast_weights.items(), gradients)}\n",
        "        task_gradients.append(named_grads)\n",
        "\n",
        "    # If performing 1st order MAML update the meta-learner weights with the gradient of the loss from part 3.\n",
        "    if order == 1:\n",
        "        if train:\n",
        "            sum_task_gradients = {k: torch.stack([grad[k] for grad in task_gradients]).mean(dim=0)\n",
        "                                  for k in task_gradients[0].keys()}\n",
        "            hooks = []\n",
        "            for name, param in model.named_parameters():\n",
        "                hooks.append(\n",
        "                    param.register_hook(replace_grad(sum_task_gradients, name))\n",
        "                )\n",
        "\n",
        "            model.train()\n",
        "            optimiser.zero_grad()\n",
        "            # Dummy pass in order to create `loss` variable\n",
        "            # Replace dummy gradients with mean task gradients using hooks\n",
        "            logits = model(torch.zeros((k_way, ) + data_shape).to(device, dtype=torch.double))\n",
        "            loss = loss_fn(logits, create_nshot_task_label(k_way, 1).to(device))\n",
        "            loss.backward()\n",
        "            optimiser.step()\n",
        "\n",
        "            for h in hooks:\n",
        "                h.remove()\n",
        "\n",
        "        return torch.stack(task_losses).mean(), torch.cat(task_predictions)\n",
        "\n",
        "    elif order == 2: # If performing 2nd order MAML calculate the derivative of the loss w.r.t. the original weights\n",
        "        model.train()\n",
        "        optimiser.zero_grad()\n",
        "        meta_batch_loss = torch.stack(task_losses).mean()\n",
        "\n",
        "        if train:\n",
        "            meta_batch_loss.backward()\n",
        "            optimiser.step()\n",
        "\n",
        "        return meta_batch_loss, torch.cat(task_predictions)\n",
        "    else:\n",
        "        raise ValueError('Order must be either 1 or 2.')"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing few_shot/maml.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBlbw63wltNf",
        "outputId": "e89c6efb-61ab-457d-9253-0ef559ecefdc"
      },
      "source": [
        "# few_shot/models.py\n",
        "%%writefile few_shot/models.py\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from typing import Dict\n",
        "\n",
        "\n",
        "##########\n",
        "# Layers #\n",
        "##########\n",
        "class Flatten(nn.Module):\n",
        "    \"\"\"Converts N-dimensional Tensor of shape [batch_size, d1, d2, ..., dn] to 2-dimensional Tensor\n",
        "    of shape [batch_size, d1*d2*...*dn].\n",
        "    # Arguments\n",
        "        input: Input tensor\n",
        "    \"\"\"\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)\n",
        "\n",
        "\n",
        "class GlobalMaxPool1d(nn.Module):\n",
        "    \"\"\"Performs global max pooling over the entire length of a batched 1D tensor\n",
        "    # Arguments\n",
        "        input: Input tensor\n",
        "    \"\"\"\n",
        "    def forward(self, input):\n",
        "        return nn.functional.max_pool1d(input, kernel_size=input.size()[2:]).view(-1, input.size(1))\n",
        "\n",
        "\n",
        "class GlobalAvgPool2d(nn.Module):\n",
        "    \"\"\"Performs global average pooling over the entire height and width of a batched 2D tensor\n",
        "    # Arguments\n",
        "        input: Input tensor\n",
        "    \"\"\"\n",
        "    def forward(self, input):\n",
        "        return nn.functional.avg_pool2d(input, kernel_size=input.size()[2:]).view(-1, input.size(1))\n",
        "\n",
        "\n",
        "def conv_block(in_channels: int, out_channels: int) -> nn.Module:\n",
        "    \"\"\"Returns a Module that performs 3x3 convolution, ReLu activation, 2x2 max pooling.\n",
        "    # Arguments\n",
        "        in_channels:\n",
        "        out_channels:\n",
        "    \"\"\"\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    )\n",
        "\n",
        "\n",
        "def functional_conv_block(x: torch.Tensor, weights: torch.Tensor, biases: torch.Tensor,\n",
        "                          bn_weights, bn_biases) -> torch.Tensor:\n",
        "    \"\"\"Performs 3x3 convolution, ReLu activation, 2x2 max pooling in a functional fashion.\n",
        "    # Arguments:\n",
        "        x: Input Tensor for the conv block\n",
        "        weights: Weights for the convolutional block\n",
        "        biases: Biases for the convolutional block\n",
        "        bn_weights:\n",
        "        bn_biases:\n",
        "    \"\"\"\n",
        "    x = F.conv2d(x, weights, biases, padding=1)\n",
        "    x = F.batch_norm(x, running_mean=None, running_var=None, weight=bn_weights, bias=bn_biases, training=True)\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "    return x\n",
        "\n",
        "\n",
        "##########\n",
        "# Models #\n",
        "##########\n",
        "def get_few_shot_encoder(num_input_channels=1) -> nn.Module:\n",
        "    \"\"\"Creates a few shot encoder as used in Matching and Prototypical Networks\n",
        "    # Arguments:\n",
        "        num_input_channels: Number of color channels the model expects input data to contain. Omniglot = 1,\n",
        "            miniImageNet = 3\n",
        "    \"\"\"\n",
        "    return nn.Sequential(\n",
        "        conv_block(num_input_channels, 64),\n",
        "        conv_block(64, 64),\n",
        "        conv_block(64, 64),\n",
        "        conv_block(64, 64),\n",
        "        Flatten(),\n",
        "    )\n",
        "\n",
        "\n",
        "class FewShotClassifier(nn.Module):\n",
        "    def __init__(self, num_input_channels: int, k_way: int, final_layer_size: int = 64):\n",
        "        \"\"\"Creates a few shot classifier as used in MAML.\n",
        "        This network should be identical to the one created by `get_few_shot_encoder` but with a\n",
        "        classification layer on top.\n",
        "        # Arguments:\n",
        "            num_input_channels: Number of color channels the model expects input data to contain. Omniglot = 1,\n",
        "                miniImageNet = 3\n",
        "            k_way: Number of classes the model will discriminate between\n",
        "            final_layer_size: 64 for Omniglot, 1600 for miniImageNet\n",
        "        \"\"\"\n",
        "        super(FewShotClassifier, self).__init__()\n",
        "        self.conv1 = conv_block(num_input_channels, 64)\n",
        "        self.conv2 = conv_block(64, 64)\n",
        "        self.conv3 = conv_block(64, 64)\n",
        "        self.conv4 = conv_block(64, 64)\n",
        "\n",
        "        self.logits = nn.Linear(final_layer_size, k_way)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        return self.logits(x)\n",
        "\n",
        "    def functional_forward(self, x, weights):\n",
        "        \"\"\"Applies the same forward pass using PyTorch functional operators using a specified set of weights.\"\"\"\n",
        "\n",
        "        for block in [1, 2, 3, 4]:\n",
        "            x = functional_conv_block(x, weights[f'conv{block}.0.weight'], weights[f'conv{block}.0.bias'],\n",
        "                                      weights.get(f'conv{block}.1.weight'), weights.get(f'conv{block}.1.bias'))\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = F.linear(x, weights['logits.weight'], weights['logits.bias'])\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class MatchingNetwork(nn.Module):\n",
        "    def __init__(self, n: int, k: int, q: int, fce: bool, num_input_channels: int,\n",
        "                 lstm_layers: int, lstm_input_size: int, unrolling_steps: int, device: torch.device):\n",
        "        \"\"\"Creates a Matching Network as described in Vinyals et al.\n",
        "        # Arguments:\n",
        "            n: Number of examples per class in the support set\n",
        "            k: Number of classes in the few shot classification task\n",
        "            q: Number of examples per class in the query set\n",
        "            fce: Whether or not to us fully conditional embeddings\n",
        "            num_input_channels: Number of color channels the model expects input data to contain. Omniglot = 1,\n",
        "                miniImageNet = 3\n",
        "            lstm_layers: Number of LSTM layers in the bidrectional LSTM g that embeds the support set (fce = True)\n",
        "            lstm_input_size: Input size for the bidirectional and Attention LSTM. This is determined by the embedding\n",
        "                dimension of the few shot encoder which is in turn determined by the size of the input data. Hence we\n",
        "                have Omniglot -> 64, miniImageNet -> 1600.\n",
        "            unrolling_steps: Number of unrolling steps to run the Attention LSTM\n",
        "            device: Device on which to run computation\n",
        "        \"\"\"\n",
        "        super(MatchingNetwork, self).__init__()\n",
        "        self.n = n\n",
        "        self.k = k\n",
        "        self.q = q\n",
        "        self.fce = fce\n",
        "        self.num_input_channels = num_input_channels\n",
        "        self.encoder = get_few_shot_encoder(self.num_input_channels)\n",
        "        if self.fce:\n",
        "            self.g = BidrectionalLSTM(lstm_input_size, lstm_layers).to(device, dtype=torch.double)\n",
        "            self.f = AttentionLSTM(lstm_input_size, unrolling_steps=unrolling_steps).to(device, dtype=torch.double)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        pass\n",
        "\n",
        "\n",
        "class BidrectionalLSTM(nn.Module):\n",
        "    def __init__(self, size: int, layers: int):\n",
        "        \"\"\"Bidirectional LSTM used to generate fully conditional embeddings (FCE) of the support set as described\n",
        "        in the Matching Networks paper.\n",
        "        # Arguments\n",
        "            size: Size of input and hidden layers. These are constrained to be the same in order to implement the skip\n",
        "                connection described in Appendix A.2\n",
        "            layers: Number of LSTM layers\n",
        "        \"\"\"\n",
        "        super(BidrectionalLSTM, self).__init__()\n",
        "        self.num_layers = layers\n",
        "        self.batch_size = 1\n",
        "        # Force input size and hidden size to be the same in order to implement\n",
        "        # the skip connection as described in Appendix A.1 and A.2 of Matching Networks\n",
        "        self.lstm = nn.LSTM(input_size=size,\n",
        "                            num_layers=layers,\n",
        "                            hidden_size=size,\n",
        "                            bidirectional=True)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Give None as initial state and Pytorch LSTM creates initial hidden states\n",
        "        output, (hn, cn) = self.lstm(inputs, None)\n",
        "\n",
        "        forward_output = output[:, :, :self.lstm.hidden_size]\n",
        "        backward_output = output[:, :, self.lstm.hidden_size:]\n",
        "\n",
        "        # g(x_i, S) = h_forward_i + h_backward_i + g'(x_i) as written in Appendix A.2\n",
        "        # AKA A skip connection between inputs and outputs is used\n",
        "        output = forward_output + backward_output + inputs\n",
        "        return output, hn, cn\n",
        "\n",
        "\n",
        "class AttentionLSTM(nn.Module):\n",
        "    def __init__(self, size: int, unrolling_steps: int):\n",
        "        \"\"\"Attentional LSTM used to generate fully conditional embeddings (FCE) of the query set as described\n",
        "        in the Matching Networks paper.\n",
        "        # Arguments\n",
        "            size: Size of input and hidden layers. These are constrained to be the same in order to implement the skip\n",
        "                connection described in Appendix A.2\n",
        "            unrolling_steps: Number of steps of attention over the support set to compute. Analogous to number of\n",
        "                layers in a regular LSTM\n",
        "        \"\"\"\n",
        "        super(AttentionLSTM, self).__init__()\n",
        "        self.unrolling_steps = unrolling_steps\n",
        "        self.lstm_cell = nn.LSTMCell(input_size=size,\n",
        "                                     hidden_size=size)\n",
        "\n",
        "    def forward(self, support, queries):\n",
        "        # Get embedding dimension, d\n",
        "        if support.shape[-1] != queries.shape[-1]:\n",
        "            raise(ValueError(\"Support and query set have different embedding dimension!\"))\n",
        "\n",
        "        batch_size = queries.shape[0]\n",
        "        embedding_dim = queries.shape[1]\n",
        "\n",
        "        h_hat = torch.zeros_like(queries).cuda().double()\n",
        "        c = torch.zeros(batch_size, embedding_dim).cuda().double()\n",
        "\n",
        "        for k in range(self.unrolling_steps):\n",
        "            # Calculate hidden state cf. equation (4) of appendix A.2\n",
        "            h = h_hat + queries\n",
        "\n",
        "            # Calculate softmax attentions between hidden states and support set embeddings\n",
        "            # cf. equation (6) of appendix A.2\n",
        "            attentions = torch.mm(h, support.t())\n",
        "            attentions = attentions.softmax(dim=1)\n",
        "\n",
        "            # Calculate readouts from support set embeddings cf. equation (5)\n",
        "            readout = torch.mm(attentions, support)\n",
        "\n",
        "            # Run LSTM cell cf. equation (3)\n",
        "            # h_hat, c = self.lstm_cell(queries, (torch.cat([h, readout], dim=1), c))\n",
        "            h_hat, c = self.lstm_cell(queries, (h + readout, c))\n",
        "\n",
        "        h = h_hat + queries\n",
        "\n",
        "        return h"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing few_shot/models.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rh7TkWb2l3DH",
        "outputId": "d03d572f-f5d2-4290-e21a-5a35a6f84d3a"
      },
      "source": [
        "# few_shot/train.py\n",
        "%%writefile few_shot/train.py\n",
        "\n",
        "\"\"\"\n",
        "The `fit` function in this file implements a slightly modified version\n",
        "of the Keras `model.fit()` API.\n",
        "\"\"\"\n",
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "from torch.nn import Module\n",
        "from torch.utils.data import DataLoader\n",
        "from typing import Callable, List, Union\n",
        "\n",
        "from few_shot.callbacks import DefaultCallback, ProgressBarLogger, CallbackList, Callback\n",
        "from few_shot.metrics import NAMED_METRICS\n",
        "\n",
        "\n",
        "def gradient_step(model: Module, optimiser: Optimizer, loss_fn: Callable, x: torch.Tensor, y: torch.Tensor, **kwargs):\n",
        "    \"\"\"Takes a single gradient step.\n",
        "    # Arguments\n",
        "        model: Model to be fitted\n",
        "        optimiser: Optimiser to calculate gradient step from loss\n",
        "        loss_fn: Loss function to calculate between predictions and outputs\n",
        "        x: Input samples\n",
        "        y: Input targets\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    optimiser.zero_grad()\n",
        "    y_pred = model(x)\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "\n",
        "    return loss, y_pred\n",
        "\n",
        "\n",
        "def batch_metrics(model: Module, y_pred: torch.Tensor, y: torch.Tensor, metrics: List[Union[str, Callable]],\n",
        "                  batch_logs: dict):\n",
        "    \"\"\"Calculates metrics for the current training batch\n",
        "    # Arguments\n",
        "        model: Model being fit\n",
        "        y_pred: predictions for a particular batch\n",
        "        y: labels for a particular batch\n",
        "        batch_logs: Dictionary of logs for the current batch\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    for m in metrics:\n",
        "        if isinstance(m, str):\n",
        "            batch_logs[m] = NAMED_METRICS[m](y, y_pred)\n",
        "        else:\n",
        "            # Assume metric is a callable function\n",
        "            batch_logs = m(y, y_pred)\n",
        "\n",
        "    return batch_logs\n",
        "\n",
        "\n",
        "def fit(model: Module, optimiser: Optimizer, loss_fn: Callable, epochs: int, dataloader: DataLoader,\n",
        "        prepare_batch: Callable, metrics: List[Union[str, Callable]] = None, callbacks: List[Callback] = None,\n",
        "        verbose: bool =True, fit_function: Callable = gradient_step, fit_function_kwargs: dict = {}):\n",
        "    \"\"\"Function to abstract away training loop.\n",
        "    The benefit of this function is that allows training scripts to be much more readable and allows for easy re-use of\n",
        "    common training functionality provided they are written as a subclass of voicemap.Callback (following the\n",
        "    Keras API).\n",
        "    # Arguments\n",
        "        model: Model to be fitted.\n",
        "        optimiser: Optimiser to calculate gradient step from loss\n",
        "        loss_fn: Loss function to calculate between predictions and outputs\n",
        "        epochs: Number of epochs of fitting to be performed\n",
        "        dataloader: `torch.DataLoader` instance to fit the model to\n",
        "        prepare_batch: Callable to perform any desired preprocessing\n",
        "        metrics: Optional list of metrics to evaluate the model with\n",
        "        callbacks: Additional functionality to incorporate into training such as logging metrics to csv, model\n",
        "            checkpointing, learning rate scheduling etc... See voicemap.callbacks for more.\n",
        "        verbose: All print output is muted if this argument is `False`\n",
        "        fit_function: Function for calculating gradients. Leave as default for simple supervised training on labelled\n",
        "            batches. For more complex training procedures (meta-learning etc...) you will need to write your own\n",
        "            fit_function\n",
        "        fit_function_kwargs: Keyword arguments to pass to `fit_function`\n",
        "    \"\"\"\n",
        "    # Determine number of samples:\n",
        "    num_batches = len(dataloader)\n",
        "    batch_size = dataloader.batch_size\n",
        "\n",
        "    callbacks = CallbackList([DefaultCallback(), ] + (callbacks or []) + [ProgressBarLogger(), ])\n",
        "    callbacks.set_model(model)\n",
        "    callbacks.set_params({\n",
        "        'num_batches': num_batches,\n",
        "        'batch_size': batch_size,\n",
        "        'verbose': verbose,\n",
        "        'metrics': (metrics or []),\n",
        "        'prepare_batch': prepare_batch,\n",
        "        'loss_fn': loss_fn,\n",
        "        'optimiser': optimiser\n",
        "    })\n",
        "\n",
        "    if verbose:\n",
        "        print('Begin training...')\n",
        "\n",
        "    callbacks.on_train_begin()\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        callbacks.on_epoch_begin(epoch)\n",
        "\n",
        "        epoch_logs = {}\n",
        "        for batch_index, batch in enumerate(dataloader):\n",
        "            batch_logs = dict(batch=batch_index, size=(batch_size or 1))\n",
        "\n",
        "            callbacks.on_batch_begin(batch_index, batch_logs)\n",
        "\n",
        "            x, y = prepare_batch(batch)\n",
        "\n",
        "            loss, y_pred = fit_function(model, optimiser, loss_fn, x, y, **fit_function_kwargs)\n",
        "            batch_logs['loss'] = loss.item()\n",
        "\n",
        "            # Loops through all metrics\n",
        "            batch_logs = batch_metrics(model, y_pred, y, metrics, batch_logs)\n",
        "\n",
        "            callbacks.on_batch_end(batch_index, batch_logs)\n",
        "\n",
        "        # Run on epoch end\n",
        "        callbacks.on_epoch_end(epoch, epoch_logs)\n",
        "\n",
        "    # Run on train end\n",
        "    if verbose:\n",
        "        print('Finished.')\n",
        "\n",
        "    callbacks.on_train_end()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing few_shot/train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyMZF6mbmKnD",
        "outputId": "d67906de-1a2b-480e-c3e6-1185790884a6"
      },
      "source": [
        "# few_shot/utils.py\n",
        "%%writefile few_shot/utils.py\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import shutil\n",
        "from typing import Tuple, List\n",
        "\n",
        "from config import EPSILON, PATH\n",
        "\n",
        "\n",
        "def mkdir(dir):\n",
        "    \"\"\"Create a directory, ignoring exceptions\n",
        "    # Arguments:\n",
        "        dir: Path of directory to create\n",
        "    \"\"\"\n",
        "    try:\n",
        "        os.mkdir(dir)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "def rmdir(dir):\n",
        "    \"\"\"Recursively remove a directory and contents, ignoring exceptions\n",
        "   # Arguments:\n",
        "       dir: Path of directory to recursively remove\n",
        "   \"\"\"\n",
        "    try:\n",
        "        shutil.rmtree(dir)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "def setup_dirs():\n",
        "    \"\"\"Creates directories for this project.\"\"\"\n",
        "    mkdir(PATH + '/logs/')\n",
        "    mkdir(PATH + '/logs/proto_nets')\n",
        "    mkdir(PATH + '/logs/matching_nets')\n",
        "    mkdir(PATH + '/logs/maml')\n",
        "    mkdir(PATH + '/models/')\n",
        "    mkdir(PATH + '/models/proto_nets')\n",
        "    mkdir(PATH + '/models/matching_nets')\n",
        "    mkdir(PATH + '/models/maml')\n",
        "\n",
        "\n",
        "def pairwise_distances(x: torch.Tensor,\n",
        "                       y: torch.Tensor,\n",
        "                       matching_fn: str) -> torch.Tensor:\n",
        "    \"\"\"Efficiently calculate pairwise distances (or other similarity scores) between\n",
        "    two sets of samples.\n",
        "    # Arguments\n",
        "        x: Query samples. A tensor of shape (n_x, d) where d is the embedding dimension\n",
        "        y: Class prototypes. A tensor of shape (n_y, d) where d is the embedding dimension\n",
        "        matching_fn: Distance metric/similarity score to compute between samples\n",
        "    \"\"\"\n",
        "    n_x = x.shape[0]\n",
        "    n_y = y.shape[0]\n",
        "\n",
        "    if matching_fn == 'l2':\n",
        "        distances = (\n",
        "                x.unsqueeze(1).expand(n_x, n_y, -1) -\n",
        "                y.unsqueeze(0).expand(n_x, n_y, -1)\n",
        "        ).pow(2).sum(dim=2)\n",
        "        return distances\n",
        "    elif matching_fn == 'cosine':\n",
        "        normalised_x = x / (x.pow(2).sum(dim=1, keepdim=True).sqrt() + EPSILON)\n",
        "        normalised_y = y / (y.pow(2).sum(dim=1, keepdim=True).sqrt() + EPSILON)\n",
        "\n",
        "        expanded_x = normalised_x.unsqueeze(1).expand(n_x, n_y, -1)\n",
        "        expanded_y = normalised_y.unsqueeze(0).expand(n_x, n_y, -1)\n",
        "\n",
        "        cosine_similarities = (expanded_x * expanded_y).sum(dim=2)\n",
        "        return 1 - cosine_similarities\n",
        "    elif matching_fn == 'dot':\n",
        "        expanded_x = x.unsqueeze(1).expand(n_x, n_y, -1)\n",
        "        expanded_y = y.unsqueeze(0).expand(n_x, n_y, -1)\n",
        "\n",
        "        return -(expanded_x * expanded_y).sum(dim=2)\n",
        "    else:\n",
        "        raise(ValueError('Unsupported similarity function'))\n",
        "\n",
        "\n",
        "def copy_weights(from_model: torch.nn.Module, to_model: torch.nn.Module):\n",
        "    \"\"\"Copies the weights from one model to another model.\n",
        "    # Arguments:\n",
        "        from_model: Model from which to source weights\n",
        "        to_model: Model which will receive weights\n",
        "    \"\"\"\n",
        "    if not from_model.__class__ == to_model.__class__:\n",
        "        raise(ValueError(\"Models don't have the same architecture!\"))\n",
        "\n",
        "    for m_from, m_to in zip(from_model.modules(), to_model.modules()):\n",
        "        is_linear = isinstance(m_to, torch.nn.Linear)\n",
        "        is_conv = isinstance(m_to, torch.nn.Conv2d)\n",
        "        is_bn = isinstance(m_to, torch.nn.BatchNorm2d)\n",
        "        if is_linear or is_conv or is_bn:\n",
        "            m_to.weight.data = m_from.weight.data.clone()\n",
        "            if m_to.bias is not None:\n",
        "                m_to.bias.data = m_from.bias.data.clone()\n",
        "\n",
        "\n",
        "def autograd_graph(tensor: torch.Tensor) -> Tuple[\n",
        "            List[torch.autograd.Function],\n",
        "            List[Tuple[torch.autograd.Function, torch.autograd.Function]]\n",
        "        ]:\n",
        "    \"\"\"Recursively retrieves the autograd graph for a particular tensor.\n",
        "    # Arguments\n",
        "        tensor: The Tensor to retrieve the autograd graph for\n",
        "    # Returns\n",
        "        nodes: List of torch.autograd.Functions that are the nodes of the autograd graph\n",
        "        edges: List of (Function, Function) tuples that are the edges between the nodes of the autograd graph\n",
        "    \"\"\"\n",
        "    nodes, edges = list(), list()\n",
        "\n",
        "    def _add_nodes(tensor):\n",
        "        if tensor not in nodes:\n",
        "            nodes.append(tensor)\n",
        "\n",
        "            if hasattr(tensor, 'next_functions'):\n",
        "                for f in tensor.next_functions:\n",
        "                    if f[0] is not None:\n",
        "                        edges.append((f[0], tensor))\n",
        "                        _add_nodes(f[0])\n",
        "\n",
        "            if hasattr(tensor, 'saved_tensors'):\n",
        "                for t in tensor.saved_tensors:\n",
        "                    edges.append((t, tensor))\n",
        "                    _add_nodes(t)\n",
        "\n",
        "    _add_nodes(tensor.grad_fn)\n",
        "\n",
        "    return nodes, edges"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting few_shot/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7xxcGEdjsu2",
        "outputId": "7f854567-537e-4c13-f5e4-9fda12265b8c"
      },
      "source": [
        "%%writefile experiments/maml.py\n",
        "\"\"\"\n",
        "Reproduce Model-agnostic Meta-learning results (supervised only) of Finn et al\n",
        "\"\"\"\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "import argparse\n",
        "\n",
        "from few_shot.datasets import OmniglotDataset, MiniImageNet\n",
        "from few_shot.core import NShotTaskSampler, create_nshot_task_label, EvaluateFewShot\n",
        "from few_shot.maml import meta_gradient_step\n",
        "from few_shot.models import FewShotClassifier\n",
        "from few_shot.train import fit\n",
        "from few_shot.callbacks import *\n",
        "from few_shot.utils import setup_dirs\n",
        "from config import PATH\n",
        "\n",
        "\n",
        "setup_dirs()\n",
        "assert torch.cuda.is_available()\n",
        "device = torch.device('cuda')\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "##############\n",
        "# Parameters #\n",
        "##############\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--dataset', default='omniglot')\n",
        "parser.add_argument('--n', default=1, type=int)\n",
        "parser.add_argument('--k', default=5, type=int)\n",
        "parser.add_argument('--q', default=1, type=int)  # Number of examples per class to calculate meta gradients with\n",
        "parser.add_argument('--inner-train-steps', default=1, type=int)\n",
        "parser.add_argument('--inner-val-steps', default=3, type=int)\n",
        "parser.add_argument('--inner-lr', default=0.4, type=float)\n",
        "parser.add_argument('--meta-lr', default=0.001, type=float)\n",
        "parser.add_argument('--meta-batch-size', default=32, type=int)\n",
        "parser.add_argument('--order', default=1, type=int)\n",
        "parser.add_argument('--epochs', default=50, type=int)\n",
        "parser.add_argument('--epoch-len', default=100, type=int)\n",
        "parser.add_argument('--eval-batches', default=20, type=int)\n",
        "\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "if args.dataset == 'omniglot':\n",
        "    dataset_class = OmniglotDataset\n",
        "    fc_layer_size = 64\n",
        "    num_input_channels = 1\n",
        "elif args.dataset == 'miniImageNet':\n",
        "    dataset_class = MiniImageNet\n",
        "    fc_layer_size = 1600\n",
        "    num_input_channels = 3\n",
        "else:\n",
        "    raise(ValueError('Unsupported dataset'))\n",
        "\n",
        "param_str = f'{args.dataset}_order={args.order}_n={args.n}_k={args.k}_metabatch={args.meta_batch_size}_' \\\n",
        "            f'train_steps={args.inner_train_steps}_val_steps={args.inner_val_steps}'\n",
        "print(param_str)\n",
        "\n",
        "\n",
        "###################\n",
        "# Create datasets #\n",
        "###################\n",
        "background = dataset_class('background')\n",
        "background_taskloader = DataLoader(\n",
        "    background,\n",
        "    batch_sampler=NShotTaskSampler(background, args.epoch_len, n=args.n, k=args.k, q=args.q,\n",
        "                                   num_tasks=args.meta_batch_size),\n",
        "    num_workers=2 # added =2 original was =8\n",
        ") \n",
        "evaluation = dataset_class('evaluation')\n",
        "evaluation_taskloader = DataLoader(\n",
        "    evaluation,\n",
        "    batch_sampler=NShotTaskSampler(evaluation, args.eval_batches, n=args.n, k=args.k, q=args.q,\n",
        "                                   num_tasks=args.meta_batch_size),\n",
        "    num_workers=2 # added =2 original was =8\n",
        ")\n",
        "\n",
        "\n",
        "############\n",
        "# Training #\n",
        "############\n",
        "print(f'Training MAML on {args.dataset}...')\n",
        "meta_model = FewShotClassifier(num_input_channels, args.k, fc_layer_size).to(device, dtype=torch.double)\n",
        "meta_optimiser = torch.optim.Adam(meta_model.parameters(), lr=args.meta_lr)\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "\n",
        "def prepare_meta_batch(n, k, q, meta_batch_size):\n",
        "    def prepare_meta_batch_(batch):\n",
        "        x, y = batch\n",
        "        # Reshape to `meta_batch_size` number of tasks. Each task contains\n",
        "        # n*k support samples to train the fast model on and q*k query samples to\n",
        "        # evaluate the fast model on and generate meta-gradients\n",
        "        x = x.reshape(meta_batch_size, n*k + q*k, num_input_channels, x.shape[-2], x.shape[-1])\n",
        "        # Move to device\n",
        "        x = x.double().to(device)\n",
        "        # Create label\n",
        "        y = create_nshot_task_label(k, q).cuda().repeat(meta_batch_size)\n",
        "        return x, y\n",
        "\n",
        "    return prepare_meta_batch_\n",
        "\n",
        "\n",
        "callbacks = [\n",
        "    EvaluateFewShot(\n",
        "        eval_fn=meta_gradient_step,\n",
        "        num_tasks=args.eval_batches,\n",
        "        n_shot=args.n,\n",
        "        k_way=args.k,\n",
        "        q_queries=args.q,\n",
        "        taskloader=evaluation_taskloader,\n",
        "        prepare_batch=prepare_meta_batch(args.n, args.k, args.q, args.meta_batch_size),\n",
        "        # MAML kwargs\n",
        "        inner_train_steps=args.inner_val_steps,\n",
        "        inner_lr=args.inner_lr,\n",
        "        device=device,\n",
        "        order=args.order,\n",
        "    ),\n",
        "    ModelCheckpoint(\n",
        "        filepath=PATH + f'/models/maml/{param_str}.pth',\n",
        "        monitor=f'val_{args.n}-shot_{args.k}-way_acc'\n",
        "    ),\n",
        "    ReduceLROnPlateau(patience=10, factor=0.5, monitor=f'val_loss'),\n",
        "    CSVLogger(PATH + f'/logs/maml/{param_str}.csv'),\n",
        "]\n",
        "\n",
        "\n",
        "fit(\n",
        "    meta_model,\n",
        "    meta_optimiser,\n",
        "    loss_fn,\n",
        "    epochs=args.epochs,\n",
        "    dataloader=background_taskloader,\n",
        "    prepare_batch=prepare_meta_batch(args.n, args.k, args.q, args.meta_batch_size),\n",
        "    callbacks=callbacks,\n",
        "    metrics=['categorical_accuracy'],\n",
        "    fit_function=meta_gradient_step,\n",
        "    fit_function_kwargs={'n_shot': args.n, 'k_way': args.k, 'q_queries': args.q,\n",
        "                         'train': True,\n",
        "                         'order': args.order, 'device': device, 'inner_train_steps': args.inner_train_steps,\n",
        "                         'inner_lr': args.inner_lr},\n",
        ")"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing experiments/maml.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsokzThV0-ML"
      },
      "source": [
        "# Training and testing options for omniglot:\n",
        "`# 1st order MAML`\n",
        "\n",
        "`!python -m experiments.maml --dataset omniglot --order 1 --n 1 --k 5 --eval-batches 10 --epoch-len 50`\n",
        "\n",
        "`!python -m experiments.maml --dataset omniglot --order 1 --n 5 --k 5 --eval-batches 10 --epoch-len 50`\n",
        "\n",
        "`!python -m experiments.maml --dataset omniglot --order 1 --n 1 --k 20 --meta-batch-size 16 --inner-train-steps 5 --inner-val-steps 5 --inner-lr 0.1 --eval-batches 20 --epoch-len 100`\n",
        "\n",
        "`!python -m experiments.maml --dataset omniglot --order 1 --n 5 --k 20 --meta-batch-size 16 --inner-train-steps 5 --inner-val-steps 5 --inner-lr 0.1 --eval-batches 20 --epoch-len 100`\n",
        "\n",
        "`# 2nd order MAML`\n",
        "\n",
        "`!python -m experiments.maml --dataset omniglot --order 2 --n 1 --k 5 --eval-batches 10 --epoch-len 50`\n",
        "\n",
        "`!python -m experiments.maml --dataset omniglot --order 2 --n 5 --k 5 --eval-batches 20 --epoch-len 100 --meta-batch-size 16 --eval-batches 20`\n",
        "\n",
        "`!python -m experiments.maml --dataset omniglot --order 2 --n 1 --k 20 --meta-batch-size 16 --inner-train-steps 5 --inner-val-steps 5 --inner-lr 0.1 --eval-batches 40 --epoch-len 200`\n",
        "\n",
        "`!python -m experiments.maml --dataset omniglot --order 2 --n 5 --k 20 --meta-batch-size 4 --inner-train-steps 5 --inner-val-steps 5 --inner-lr 0.1 --eval-batches 80 --epoch-len 400`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyvT8JRB15G2"
      },
      "source": [
        "# Training and testing options for MiniImageNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJ8QTbsZ2ATB"
      },
      "source": [
        "`# 1st order MAML`\n",
        "\n",
        "`!python -m experiments.maml --dataset miniImageNet --order 1 --n 1 --k 5 --q 5 --meta-batch-size 4 --inner-train-steps 5 --inner-val-steps 10 --inner-lr 0.01 --eval-batches 40 --epoch-len 400`\n",
        "\n",
        "`!python -m experiments.maml --dataset miniImageNet --order 1 --n 5 --k 5 --q 5 --meta-batch-size 4 --inner-train-steps 5 --inner-val-steps 10 --inner-lr 0.01 --eval-batches 40 --epoch-len 400`\n",
        "\n",
        "`# 2nd order MAML`\n",
        "\n",
        "`!python -m experiments.maml --dataset miniImageNet --order 1 --n 1 --k 5 --q 5 --meta-batch-size 4 --inner-train-steps 5 --inner-val-steps 10 --inner-lr 0.01 --eval-batches 40 --epoch-len 400`\n",
        "\n",
        "`!python -m experiments.maml --dataset miniImageNet --order 1 --n 5 --k 5 --q 5 --meta-batch-size 4 --inner-train-steps 5 --inner-val-steps 10 --inner-lr 0.01 --eval-batches 40 --epoch-len 400`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rh4HslbasHBr",
        "outputId": "d0a74117-c1da-4567-95ef-1305946d4297"
      },
      "source": [
        "# 1st order MAML\n",
        "!python -m experiments.maml --dataset omniglot --order 1 --n 1 --k 5 --eval-batches 10 --epoch-len 50"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "omniglot_order=1_n=1_k=5_metabatch=32_train_steps=1_val_steps=3\n",
            "Indexing background...\n",
            "100% 77120/77120 [00:00<00:00, 182407.63it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Indexing evaluation...\n",
            "100% 52720/52720 [00:00<00:00, 207257.55it/s]\n",
            "Training MAML on omniglot...\n",
            "Begin training...\n",
            "Epoch 1:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "Epoch 1:  98% 49/50 [00:40<00:00,  3.59it/s, loss=1.43, categorical_accuracy=0.487]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 1: 100% 50/50 [00:51<00:00,  1.03s/it, loss=1.55, categorical_accuracy=0.397]\n",
            "Epoch 2:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 2:  98% 49/50 [00:40<00:00,  3.60it/s, loss=1.36, categorical_accuracy=0.494]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 2: 100% 50/50 [00:51<00:00,  1.03s/it, loss=1.37, categorical_accuracy=0.437]\n",
            "Epoch 3:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 3:  98% 49/50 [00:40<00:00,  3.57it/s, loss=1.36, categorical_accuracy=0.431]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 3: 100% 50/50 [00:51<00:00,  1.03s/it, loss=1.37, categorical_accuracy=0.44]\n",
            "Epoch 4:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 4:  98% 49/50 [00:40<00:00,  3.53it/s, loss=1.35, categorical_accuracy=0.406]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 4: 100% 50/50 [00:51<00:00,  1.03s/it, loss=1.36, categorical_accuracy=0.435]\n",
            "Epoch 5:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 5:  98% 49/50 [00:40<00:00,  3.59it/s, loss=1.27, categorical_accuracy=0.487]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 5: 100% 50/50 [00:51<00:00,  1.04s/it, loss=1.3, categorical_accuracy=0.45]\n",
            "Epoch 6:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 6:  98% 49/50 [00:41<00:00,  3.60it/s, loss=1.21, categorical_accuracy=0.475]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 6: 100% 50/50 [00:51<00:00,  1.04s/it, loss=1.25, categorical_accuracy=0.47]\n",
            "Epoch 7:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 7:  98% 49/50 [00:40<00:00,  3.59it/s, loss=1.28, categorical_accuracy=0.444]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 7: 100% 50/50 [00:51<00:00,  1.03s/it, loss=1.23, categorical_accuracy=0.493]\n",
            "Epoch 8:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 8:  98% 49/50 [00:40<00:00,  3.59it/s, loss=1.16, categorical_accuracy=0.569]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 8: 100% 50/50 [00:51<00:00,  1.03s/it, loss=1.18, categorical_accuracy=0.536]\n",
            "Epoch 9:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 9:  98% 49/50 [00:40<00:00,  3.59it/s, loss=1.15, categorical_accuracy=0.525]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 9: 100% 50/50 [00:51<00:00,  1.03s/it, loss=1.15, categorical_accuracy=0.568]\n",
            "Epoch 10:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 10:  98% 49/50 [00:40<00:00,  3.60it/s, loss=1.13, categorical_accuracy=0.581]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 10: 100% 50/50 [00:51<00:00,  1.03s/it, loss=1.09, categorical_accuracy=0.6]\n",
            "Epoch 11:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 11:  98% 49/50 [00:40<00:00,  3.54it/s, loss=1.11, categorical_accuracy=0.562]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 11: 100% 50/50 [00:51<00:00,  1.04s/it, loss=1.08, categorical_accuracy=0.605]\n",
            "Epoch 12:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 12:  98% 49/50 [00:40<00:00,  3.58it/s, loss=1.01, categorical_accuracy=0.65]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 12: 100% 50/50 [00:51<00:00,  1.03s/it, loss=1.04, categorical_accuracy=0.62]\n",
            "Epoch 13:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 13:  98% 49/50 [00:40<00:00,  3.57it/s, loss=1.02, categorical_accuracy=0.631] /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 13: 100% 50/50 [00:51<00:00,  1.03s/it, loss=1.01, categorical_accuracy=0.633]\n",
            "Epoch 14:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 14:  98% 49/50 [00:40<00:00,  3.57it/s, loss=1.04, categorical_accuracy=0.613]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 14: 100% 50/50 [00:51<00:00,  1.03s/it, loss=0.981, categorical_accuracy=0.636]\n",
            "Epoch 15:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 15:  98% 49/50 [00:40<00:00,  3.58it/s, loss=0.98, categorical_accuracy=0.625] /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 15: 100% 50/50 [00:51<00:00,  1.03s/it, loss=0.964, categorical_accuracy=0.64]\n",
            "Epoch 16:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 16:  98% 49/50 [00:40<00:00,  3.56it/s, loss=0.876, categorical_accuracy=0.662]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 16: 100% 50/50 [00:51<00:00,  1.03s/it, loss=0.93, categorical_accuracy=0.643]\n",
            "Epoch 17:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 17:  98% 49/50 [00:40<00:00,  3.52it/s, loss=0.893, categorical_accuracy=0.669]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 17: 100% 50/50 [00:51<00:00,  1.04s/it, loss=0.9, categorical_accuracy=0.656]\n",
            "Epoch 18:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 18:  98% 49/50 [00:40<00:00,  3.58it/s, loss=0.977, categorical_accuracy=0.631]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 18: 100% 50/50 [00:51<00:00,  1.04s/it, loss=0.863, categorical_accuracy=0.679]\n",
            "Epoch 19:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 19:  98% 49/50 [00:40<00:00,  3.56it/s, loss=0.874, categorical_accuracy=0.662]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 19: 100% 50/50 [00:51<00:00,  1.03s/it, loss=0.867, categorical_accuracy=0.681]\n",
            "Epoch 20:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 20:  98% 49/50 [00:40<00:00,  3.52it/s, loss=0.85, categorical_accuracy=0.713]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 20: 100% 50/50 [00:51<00:00,  1.03s/it, loss=0.845, categorical_accuracy=0.696]\n",
            "Epoch 21:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 21:  98% 49/50 [00:40<00:00,  3.52it/s, loss=0.775, categorical_accuracy=0.75] /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 21: 100% 50/50 [00:51<00:00,  1.03s/it, loss=0.819, categorical_accuracy=0.706]\n",
            "Epoch 22:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 22:  98% 49/50 [00:40<00:00,  3.49it/s, loss=0.806, categorical_accuracy=0.756]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 22: 100% 50/50 [00:51<00:00,  1.03s/it, loss=0.804, categorical_accuracy=0.724]\n",
            "Epoch 23:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 23:  98% 49/50 [00:40<00:00,  3.47it/s, loss=0.765, categorical_accuracy=0.719]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 23: 100% 50/50 [00:51<00:00,  1.04s/it, loss=0.776, categorical_accuracy=0.734]\n",
            "Epoch 24:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 24:  98% 49/50 [00:40<00:00,  3.54it/s, loss=0.667, categorical_accuracy=0.781]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 24: 100% 50/50 [00:51<00:00,  1.03s/it, loss=0.743, categorical_accuracy=0.749]\n",
            "Epoch 25:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 25:  98% 49/50 [00:40<00:00,  3.48it/s, loss=0.695, categorical_accuracy=0.781]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 25: 100% 50/50 [00:51<00:00,  1.03s/it, loss=0.707, categorical_accuracy=0.759]\n",
            "Epoch 26:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 26:  98% 49/50 [00:40<00:00,  3.50it/s, loss=0.71, categorical_accuracy=0.744] /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 26: 100% 50/50 [00:51<00:00,  1.04s/it, loss=0.702, categorical_accuracy=0.757]\n",
            "Epoch 27:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 27:  98% 49/50 [00:40<00:00,  3.50it/s, loss=0.648, categorical_accuracy=0.769]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 27: 100% 50/50 [00:51<00:00,  1.04s/it, loss=0.662, categorical_accuracy=0.771]\n",
            "Epoch 28:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 28:  98% 49/50 [00:40<00:00,  3.52it/s, loss=0.596, categorical_accuracy=0.838]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 28: 100% 50/50 [00:51<00:00,  1.03s/it, loss=0.624, categorical_accuracy=0.788]\n",
            "Epoch 29:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 29:  98% 49/50 [00:40<00:00,  3.51it/s, loss=0.508, categorical_accuracy=0.819]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 29: 100% 50/50 [00:51<00:00,  1.04s/it, loss=0.613, categorical_accuracy=0.791]\n",
            "Epoch 30:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 30:  98% 49/50 [00:40<00:00,  3.49it/s, loss=0.501, categorical_accuracy=0.831]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 30: 100% 50/50 [00:51<00:00,  1.04s/it, loss=0.61, categorical_accuracy=0.79]\n",
            "Epoch 31:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 31:  98% 49/50 [00:40<00:00,  3.55it/s, loss=0.533, categorical_accuracy=0.819]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 31: 100% 50/50 [00:51<00:00,  1.03s/it, loss=0.581, categorical_accuracy=0.803]\n",
            "Epoch 32:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 32:  98% 49/50 [00:40<00:00,  3.51it/s, loss=0.542, categorical_accuracy=0.838]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 32: 100% 50/50 [00:51<00:00,  1.04s/it, loss=0.574, categorical_accuracy=0.808]\n",
            "Epoch 33:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 33:  98% 49/50 [00:40<00:00,  3.53it/s, loss=0.429, categorical_accuracy=0.831]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 33: 100% 50/50 [00:51<00:00,  1.04s/it, loss=0.538, categorical_accuracy=0.819]\n",
            "Epoch 34:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 34:  98% 49/50 [00:40<00:00,  3.55it/s, loss=0.455, categorical_accuracy=0.875]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 34: 100% 50/50 [00:51<00:00,  1.03s/it, loss=0.522, categorical_accuracy=0.824]\n",
            "Epoch 35:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 35:  98% 49/50 [00:40<00:00,  3.49it/s, loss=0.536, categorical_accuracy=0.812]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 35: 100% 50/50 [00:51<00:00,  1.03s/it, loss=0.523, categorical_accuracy=0.823]\n",
            "Epoch 36:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 36:  98% 49/50 [00:40<00:00,  3.48it/s, loss=0.505, categorical_accuracy=0.812]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 36: 100% 50/50 [00:51<00:00,  1.03s/it, loss=0.492, categorical_accuracy=0.836]\n",
            "Epoch 37:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 37:  98% 49/50 [00:40<00:00,  3.53it/s, loss=0.488, categorical_accuracy=0.838]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 37: 100% 50/50 [00:51<00:00,  1.03s/it, loss=0.487, categorical_accuracy=0.836]\n",
            "Epoch 38:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 38:  98% 49/50 [00:40<00:00,  3.47it/s, loss=0.441, categorical_accuracy=0.863]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 38: 100% 50/50 [00:51<00:00,  1.04s/it, loss=0.492, categorical_accuracy=0.834]\n",
            "Epoch 39:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 39:  98% 49/50 [00:40<00:00,  3.46it/s, loss=0.574, categorical_accuracy=0.8] /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 39: 100% 50/50 [00:51<00:00,  1.03s/it, loss=0.471, categorical_accuracy=0.847]\n",
            "Epoch 40:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 40:  98% 49/50 [00:40<00:00,  3.46it/s, loss=0.431, categorical_accuracy=0.85] /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 40: 100% 50/50 [00:51<00:00,  1.03s/it, loss=0.451, categorical_accuracy=0.848]\n",
            "Epoch 41:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 41:  98% 49/50 [00:41<00:00,  3.53it/s, loss=0.42, categorical_accuracy=0.869] /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 41: 100% 50/50 [00:52<00:00,  1.04s/it, loss=0.433, categorical_accuracy=0.862]\n",
            "Epoch 42:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 42:  98% 49/50 [00:40<00:00,  3.49it/s, loss=0.464, categorical_accuracy=0.812]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 42: 100% 50/50 [00:51<00:00,  1.04s/it, loss=0.436, categorical_accuracy=0.855]\n",
            "Epoch 43:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 43:  98% 49/50 [00:40<00:00,  3.53it/s, loss=0.352, categorical_accuracy=0.9]  /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 43: 100% 50/50 [00:51<00:00,  1.03s/it, loss=0.418, categorical_accuracy=0.863]\n",
            "Epoch 44:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 44:  98% 49/50 [00:40<00:00,  3.50it/s, loss=0.351, categorical_accuracy=0.9]  /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 44: 100% 50/50 [00:52<00:00,  1.04s/it, loss=0.406, categorical_accuracy=0.867]\n",
            "Epoch 45:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 45:  98% 49/50 [00:41<00:00,  3.52it/s, loss=0.365, categorical_accuracy=0.863]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 45: 100% 50/50 [00:52<00:00,  1.04s/it, loss=0.401, categorical_accuracy=0.87]\n",
            "Epoch 46:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 46:  98% 49/50 [00:41<00:00,  3.57it/s, loss=0.391, categorical_accuracy=0.887]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 46: 100% 50/50 [00:52<00:00,  1.04s/it, loss=0.395, categorical_accuracy=0.871]\n",
            "Epoch 47:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 47:  98% 49/50 [00:40<00:00,  3.51it/s, loss=0.338, categorical_accuracy=0.9]  /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 47: 100% 50/50 [00:51<00:00,  1.04s/it, loss=0.379, categorical_accuracy=0.876]\n",
            "Epoch 48:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 48:  98% 49/50 [00:40<00:00,  3.57it/s, loss=0.312, categorical_accuracy=0.894]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 48: 100% 50/50 [00:51<00:00,  1.04s/it, loss=0.378, categorical_accuracy=0.876]\n",
            "Epoch 49:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 49:  98% 49/50 [00:40<00:00,  3.54it/s, loss=0.351, categorical_accuracy=0.906]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 49: 100% 50/50 [00:51<00:00,  1.04s/it, loss=0.382, categorical_accuracy=0.87]\n",
            "Epoch 50:   0% 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 50:  98% 49/50 [00:40<00:00,  3.51it/s, loss=0.376, categorical_accuracy=0.881]/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Epoch 50: 100% 50/50 [00:51<00:00,  1.04s/it, loss=0.368, categorical_accuracy=0.881]\n",
            "Finished.\n"
          ]
        }
      ]
    }
  ]
}
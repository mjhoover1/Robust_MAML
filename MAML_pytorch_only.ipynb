{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from itertools import compress\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[4, 0, 1, 2, 2, 4, 2, 2, 1, 1, 0, 4, 3, 0, 3, 4, 2, 0, 1, 1, 4, 0, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "training_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "class_choosen = [0,1,2,3,4]\n",
    "boolean_array_train = [target in class_choosen for target in training_data.targets]\n",
    "boolean_array_test = [target in class_choosen for target in test_data.targets]\n",
    "training_data.data = list(compress(training_data.data,boolean_array_train))\n",
    "training_data.targets = list(compress(training_data.targets,boolean_array_train))\n",
    "test_data.data = list(compress(training_data.data,boolean_array_train))\n",
    "test_data.targets = list(compress(training_data.targets,boolean_array_train))\n",
    "Task_1 = {}\n",
    "Task_1['query'] = randomSelectSamples(test_data, 5)\n",
    "Task_1['support'] = randomSelectSamplesByClasses(training_data, class_choosen,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 25000\n",
      "    Root location: data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 n-way 5 k-shot\n",
    "def randomSelectSamples(data, samples):\n",
    "\n",
    "    randomlist = random.sample(range(0, len(data.targets)-1), min(len(data.targets), samples))\n",
    "    data.targets = [data.targets[i] for i in randomlist]\n",
    "    data.data = [data.data[i] for i in randomlist]\n",
    "    \n",
    "    return data\n",
    "\n",
    "def randomSelectSamplesByClasses(data, classes, sample_size):\n",
    "\n",
    "    randomlist = random.sample(range(0, len(data.targets)), min(len(data.targets), 100*len(classes)*sample_size))\n",
    "    counter_by_class = {}\n",
    "    new_index_list = []\n",
    "    for i in randomlist:\n",
    "        if(data.targets[i] not in counter_by_class):\n",
    "            counter_by_class[data.targets[i]] = 1\n",
    "            new_index_list.append(i)\n",
    "        \n",
    "        elif(counter_by_class[data.targets[i]] < sample_size):\n",
    "            counter_by_class[data.targets[i]] += 1\n",
    "            new_index_list.append(i)\n",
    "            \n",
    "    data.targets = [data.targets[i] for i in new_index_list]\n",
    "    data.data = [data.data[i] for i in new_index_list]\n",
    "    print(data.targets)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 10, 1]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(range(0,20), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:40: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:49: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:58: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:67: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:95: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:100: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:105: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:109: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:114: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:117: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:149: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:155: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:161: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:166: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:173: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:176: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:179: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:181: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:183: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:185: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:187: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:189: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:191: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:40: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:49: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:58: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:67: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:95: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:100: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:105: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:109: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:114: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:117: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:149: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:155: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:161: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:166: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:173: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:176: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:179: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:181: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:183: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:185: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:187: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:189: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:191: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-140-459d2af31466>:40: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if name is 'conv2d':\n",
      "<ipython-input-140-459d2af31466>:49: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'convt2d':\n",
      "<ipython-input-140-459d2af31466>:58: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'linear':\n",
      "<ipython-input-140-459d2af31466>:67: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'bn':\n",
      "<ipython-input-140-459d2af31466>:95: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if name is 'conv2d':\n",
      "<ipython-input-140-459d2af31466>:100: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'convt2d':\n",
      "<ipython-input-140-459d2af31466>:105: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'linear':\n",
      "<ipython-input-140-459d2af31466>:109: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'leakyrelu':\n",
      "<ipython-input-140-459d2af31466>:114: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'avg_pool2d':\n",
      "<ipython-input-140-459d2af31466>:117: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'max_pool2d':\n",
      "<ipython-input-140-459d2af31466>:149: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if name is 'conv2d':\n",
      "<ipython-input-140-459d2af31466>:155: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'convt2d':\n",
      "<ipython-input-140-459d2af31466>:161: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'linear':\n",
      "<ipython-input-140-459d2af31466>:166: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'bn':\n",
      "<ipython-input-140-459d2af31466>:173: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'flatten':\n",
      "<ipython-input-140-459d2af31466>:176: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'reshape':\n",
      "<ipython-input-140-459d2af31466>:179: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'relu':\n",
      "<ipython-input-140-459d2af31466>:181: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'leakyrelu':\n",
      "<ipython-input-140-459d2af31466>:183: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'tanh':\n",
      "<ipython-input-140-459d2af31466>:185: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'sigmoid':\n",
      "<ipython-input-140-459d2af31466>:187: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'upsample':\n",
      "<ipython-input-140-459d2af31466>:189: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'max_pool2d':\n",
      "<ipython-input-140-459d2af31466>:191: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'avg_pool2d':\n"
     ]
    }
   ],
   "source": [
    "import  torch\n",
    "from    torch import nn\n",
    "from    torch.nn import functional as F\n",
    "import  numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# \"\"\"Taken From:\n",
    "\n",
    "# @misc{MAML_Pytorch,\n",
    "#   author = {Liangqu Long},\n",
    "#   title = {MAML-Pytorch Implementation},\n",
    "#   year = {2018},\n",
    "#   publisher = {GitHub},\n",
    "#   journal = {GitHub repository},\n",
    "#   howpublished = {\\url{https://github.com/dragen1860/MAML-Pytorch}},\n",
    "#   commit = {master}\n",
    "# }\n",
    "\n",
    "# \"\"\"\n",
    "# import torch.nn.functional as F\n",
    "class Learner(nn.Module):\n",
    "    def __init__(self, config, imgc, imgsz):\n",
    "        \"\"\"\n",
    "        :param config: network config file, type:list of (string, list)\n",
    "        :param imgc: 1 or 3\n",
    "        :param imgsz:  28 or 84\n",
    "        \"\"\"\n",
    "        super(Learner, self).__init__()\n",
    "\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        # this dict contains all tensors needed to be optimized\n",
    "        self.vars = nn.ParameterList()\n",
    "        # running_mean and running_var\n",
    "        self.vars_bn = nn.ParameterList()\n",
    "\n",
    "        for i, (name, param) in enumerate(self.config):\n",
    "            if name is 'conv2d':\n",
    "                # [ch_out, ch_in, kernelsz, kernelsz]\n",
    "                w = nn.Parameter(torch.ones(*param[:4]))\n",
    "                # gain=1 according to cbfin's implementation\n",
    "                torch.nn.init.kaiming_normal_(w)\n",
    "                self.vars.append(w)\n",
    "                # [ch_out]\n",
    "                self.vars.append(nn.Parameter(torch.zeros(param[0])))\n",
    "\n",
    "            elif name is 'convt2d':\n",
    "                # [ch_in, ch_out, kernelsz, kernelsz, stride, padding]\n",
    "                w = nn.Parameter(torch.ones(*param[:4]))\n",
    "                # gain=1 according to cbfin's implementation\n",
    "                torch.nn.init.kaiming_normal_(w)\n",
    "                self.vars.append(w)\n",
    "                # [ch_in, ch_out]\n",
    "                self.vars.append(nn.Parameter(torch.zeros(param[1])))\n",
    "\n",
    "            elif name is 'linear':\n",
    "                # [ch_out, ch_in]\n",
    "                w = nn.Parameter(torch.ones(*param))\n",
    "                # gain=1 according to cbfinn's implementation\n",
    "                torch.nn.init.kaiming_normal_(w)\n",
    "                self.vars.append(w)\n",
    "                # [ch_out]\n",
    "                self.vars.append(nn.Parameter(torch.zeros(param[0])))\n",
    "\n",
    "            elif name is 'bn':\n",
    "                # [ch_out]\n",
    "                w = nn.Parameter(torch.ones(param[0]))\n",
    "                self.vars.append(w)\n",
    "                # [ch_out]\n",
    "                self.vars.append(nn.Parameter(torch.zeros(param[0])))\n",
    "\n",
    "                # must set requires_grad=False\n",
    "                running_mean = nn.Parameter(torch.zeros(param[0]), requires_grad=False)\n",
    "                running_var = nn.Parameter(torch.ones(param[0]), requires_grad=False)\n",
    "                self.vars_bn.extend([running_mean, running_var])\n",
    "\n",
    "\n",
    "            elif name in ['tanh', 'relu', 'upsample', 'avg_pool2d', 'max_pool2d',\n",
    "                          'flatten', 'reshape', 'leakyrelu', 'sigmoid']:\n",
    "                continue\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def extra_repr(self):\n",
    "        info = ''\n",
    "\n",
    "        for name, param in self.config:\n",
    "            if name is 'conv2d':\n",
    "                tmp = 'conv2d:(ch_in:%d, ch_out:%d, k:%dx%d, stride:%d, padding:%d)'\\\n",
    "                      %(param[1], param[0], param[2], param[3], param[4], param[5],)\n",
    "                info += tmp + '\\n'\n",
    "\n",
    "            elif name is 'convt2d':\n",
    "                tmp = 'convTranspose2d:(ch_in:%d, ch_out:%d, k:%dx%d, stride:%d, padding:%d)'\\\n",
    "                      %(param[0], param[1], param[2], param[3], param[4], param[5],)\n",
    "                info += tmp + '\\n'\n",
    "\n",
    "            elif name is 'linear':\n",
    "                tmp = 'linear:(in:%d, out:%d)'%(param[1], param[0])\n",
    "                info += tmp + '\\n'\n",
    "\n",
    "            elif name is 'leakyrelu':\n",
    "                tmp = 'leakyrelu:(slope:%f)'%(param[0])\n",
    "                info += tmp + '\\n'\n",
    "\n",
    "\n",
    "            elif name is 'avg_pool2d':\n",
    "                tmp = 'avg_pool2d:(k:%d, stride:%d, padding:%d)'%(param[0], param[1], param[2])\n",
    "                info += tmp + '\\n'\n",
    "            elif name is 'max_pool2d':\n",
    "                tmp = 'max_pool2d:(k:%d, stride:%d, padding:%d)'%(param[0], param[1], param[2])\n",
    "                info += tmp + '\\n'\n",
    "            elif name in ['flatten', 'tanh', 'relu', 'upsample', 'reshape', 'sigmoid', 'use_logits', 'bn']:\n",
    "                tmp = name + ':' + str(tuple(param))\n",
    "                info += tmp + '\\n'\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        return info\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, vars=None, bn_training=True):\n",
    "        \"\"\"\n",
    "        This function can be called by finetunning, however, in finetunning, we dont wish to update\n",
    "        running_mean/running_var. Thought weights/bias of bn is updated, it has been separated by fast_weights.\n",
    "        Indeed, to not update running_mean/running_var, we need set update_bn_statistics=False\n",
    "        but weight/bias will be updated and not dirty initial theta parameters via fast_weiths.\n",
    "        :param x: [b, 1, 28, 28]\n",
    "        :param vars:\n",
    "        :param bn_training: set False to not update\n",
    "        :return: x, loss, likelihood, kld\n",
    "        \"\"\"\n",
    "\n",
    "        if vars is None:\n",
    "            vars = self.vars\n",
    "\n",
    "        idx = 0\n",
    "        bn_idx = 0\n",
    "\n",
    "        for name, param in self.config:\n",
    "            if name is 'conv2d':\n",
    "                w, b = vars[idx], vars[idx + 1]\n",
    "                # remember to keep synchrozied of forward_encoder and forward_decoder!\n",
    "                x = F.conv2d(x, w, b, stride=param[4], padding=param[5])\n",
    "                idx += 2\n",
    "                # print(name, param, '\\tout:', x.shape)\n",
    "            elif name is 'convt2d':\n",
    "                w, b = vars[idx], vars[idx + 1]\n",
    "                # remember to keep synchrozied of forward_encoder and forward_decoder!\n",
    "                x = F.conv_transpose2d(x, w, b, stride=param[4], padding=param[5])\n",
    "                idx += 2\n",
    "                # print(name, param, '\\tout:', x.shape)\n",
    "            elif name is 'linear':\n",
    "                w, b = vars[idx], vars[idx + 1]\n",
    "                x = F.linear(x, w, b)\n",
    "                idx += 2\n",
    "                # print('forward:', idx, x.norm().item())\n",
    "            elif name is 'bn':\n",
    "                w, b = vars[idx], vars[idx + 1]\n",
    "                running_mean, running_var = self.vars_bn[bn_idx], self.vars_bn[bn_idx+1]\n",
    "                x = F.batch_norm(x, running_mean, running_var, weight=w, bias=b, training=bn_training)\n",
    "                idx += 2\n",
    "                bn_idx += 2\n",
    "\n",
    "            elif name is 'flatten':\n",
    "                # print(x.shape)\n",
    "                x = x.view(x.size(0), -1)\n",
    "            elif name is 'reshape':\n",
    "                # [b, 8] => [b, 2, 2, 2]\n",
    "                x = x.view(x.size(0), *param)\n",
    "            elif name is 'relu':\n",
    "                x = F.relu(x, inplace=param[0])\n",
    "            elif name is 'leakyrelu':\n",
    "                x = F.leaky_relu(x, negative_slope=param[0], inplace=param[1])\n",
    "            elif name is 'tanh':\n",
    "                x = F.tanh(x)\n",
    "            elif name is 'sigmoid':\n",
    "                x = torch.sigmoid(x)\n",
    "            elif name is 'upsample':\n",
    "                x = F.upsample_nearest(x, scale_factor=param[0])\n",
    "            elif name is 'max_pool2d':\n",
    "                x = F.max_pool2d(x, param[0], param[1], param[2])\n",
    "            elif name is 'avg_pool2d':\n",
    "                x = F.avg_pool2d(x, param[0], param[1], param[2])\n",
    "\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        # make sure variable is used properly\n",
    "        assert idx == len(vars)\n",
    "        assert bn_idx == len(self.vars_bn)\n",
    "\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def zero_grad(self, vars=None):\n",
    "        \"\"\"\n",
    "        :param vars:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            if vars is None:\n",
    "                for p in self.vars:\n",
    "                    if p.grad is not None:\n",
    "                        p.grad.zero_()\n",
    "            else:\n",
    "                for p in vars:\n",
    "                    if p.grad is not None:\n",
    "                        p.grad.zero_()\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        override this function since initial parameters will return with a generator.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[228 222 236]\n",
      "  [224 213 222]\n",
      "  [212 206 207]\n",
      "  ...\n",
      "  [141 129 123]\n",
      "  [ 87  73  71]\n",
      "  [ 50  43  44]]\n",
      "\n",
      " [[233 228 241]\n",
      "  [224 213 223]\n",
      "  [215 209 211]\n",
      "  ...\n",
      "  [125 113 110]\n",
      "  [ 58  42  40]\n",
      "  [ 62  52  48]]\n",
      "\n",
      " [[234 229 242]\n",
      "  [224 213 222]\n",
      "  [218 212 213]\n",
      "  ...\n",
      "  [144 134 132]\n",
      "  [ 82  65  61]\n",
      "  [ 99  85  73]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[108 108 107]\n",
      "  [119 119 117]\n",
      "  [125 125 123]\n",
      "  ...\n",
      "  [146 149 154]\n",
      "  [144 147 153]\n",
      "  [134 137 143]]\n",
      "\n",
      " [[119 119 119]\n",
      "  [122 122 122]\n",
      "  [124 124 124]\n",
      "  ...\n",
      "  [147 149 158]\n",
      "  [130 133 142]\n",
      "  [110 113 122]]\n",
      "\n",
      " [[130 129 132]\n",
      "  [130 129 132]\n",
      "  [125 125 127]\n",
      "  ...\n",
      "  [103 107 111]\n",
      "  [ 60  63  67]\n",
      "  [ 43  47  51]]]\n"
     ]
    }
   ],
   "source": [
    "dataloader_spt = DataLoader(Task_1['support'], batch_size = 25)\n",
    "dataloader_qry = DataLoader(Task_1['query'], batch_size =5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ways = 5\n",
    "# [ch_in, ch_out, kernelsz, kernelsz, stride, padding]\n",
    "\n",
    "config = [\n",
    "    ('conv2d', [25, 3, 3, 3, 2, 0]),\n",
    "    ('relu', [True]),\n",
    "    ('bn', [25]),\n",
    "    ('conv2d', [25, 25, 3, 3, 2, 0]),\n",
    "    ('relu', [True]),\n",
    "    ('bn', [25]),\n",
    "    ('conv2d', [25, 25, 3, 3, 2, 0]),\n",
    "    ('relu', [True]),\n",
    "    ('bn', [25]),\n",
    "    ('conv2d', [25, 25, 2, 2, 1, 0]),\n",
    "    ('relu', [True]),\n",
    "    ('bn', [25]),\n",
    "    ('flatten', []),\n",
    "    ('linear', [n_ways, 100])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "model = Learner(config,3,32)\n",
    "batch = [(dataloader_spt,dataloader_qry)]\n",
    "meta_lr = 1e-3\n",
    "meta_opt = torch.optim.SGD(model.parameters(), lr = 1e-3)\n",
    "for batch in task_batch:    \n",
    "    outer_loss = 0\n",
    "    for task in batch:\n",
    "        train_inputs, train_targets = iter(task[0]).next()\n",
    "        test_inputs, test_targets = iter(task[1]).next()\n",
    "        train_logits = model(train_inputs)\n",
    "        inner_loss = F.cross_entropy(train_logits, train_targets)\n",
    "        model.zero_grad()\n",
    "        grad = torch.autograd.grad(inner_loss, model.parameters())\n",
    "        fast_weights = list(map(lambda p: p[1] - meta_lr * p[0], zip(grad, model.parameters())))\n",
    "\n",
    "        test_logits = model(test_inputs, fast_weights)\n",
    "        outer_loss += F.cross_entropy(test_logits, test_targets)\n",
    "\n",
    "    meta_opt.zero_grad()\n",
    "    outer_loss.backward()\n",
    "    meta_opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

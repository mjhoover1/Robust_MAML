{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from    torch import nn\n",
    "from    torch.nn import functional as F\n",
    "import  numpy as np\n",
    "from MiniImagenet import MiniImagenet\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the config and get the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:24: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:33: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:42: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:51: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:79: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:84: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:89: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:93: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:98: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:101: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:133: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:139: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:145: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:150: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:157: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:160: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:163: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:165: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:167: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:169: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:171: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:173: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:175: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:24: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:33: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:42: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:51: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:79: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:84: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:89: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:93: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:98: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:101: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:133: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:139: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:145: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:150: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:157: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:160: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:163: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:165: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:167: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:169: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:171: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:173: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:175: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-17-53a86e708621>:24: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if name is 'conv2d':\n",
      "<ipython-input-17-53a86e708621>:33: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'convt2d':\n",
      "<ipython-input-17-53a86e708621>:42: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'linear':\n",
      "<ipython-input-17-53a86e708621>:51: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'bn':\n",
      "<ipython-input-17-53a86e708621>:79: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if name is 'conv2d':\n",
      "<ipython-input-17-53a86e708621>:84: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'convt2d':\n",
      "<ipython-input-17-53a86e708621>:89: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'linear':\n",
      "<ipython-input-17-53a86e708621>:93: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'leakyrelu':\n",
      "<ipython-input-17-53a86e708621>:98: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'avg_pool2d':\n",
      "<ipython-input-17-53a86e708621>:101: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'max_pool2d':\n",
      "<ipython-input-17-53a86e708621>:133: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if name is 'conv2d':\n",
      "<ipython-input-17-53a86e708621>:139: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'convt2d':\n",
      "<ipython-input-17-53a86e708621>:145: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'linear':\n",
      "<ipython-input-17-53a86e708621>:150: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'bn':\n",
      "<ipython-input-17-53a86e708621>:157: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'flatten':\n",
      "<ipython-input-17-53a86e708621>:160: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'reshape':\n",
      "<ipython-input-17-53a86e708621>:163: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'relu':\n",
      "<ipython-input-17-53a86e708621>:165: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'leakyrelu':\n",
      "<ipython-input-17-53a86e708621>:167: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'tanh':\n",
      "<ipython-input-17-53a86e708621>:169: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'sigmoid':\n",
      "<ipython-input-17-53a86e708621>:171: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'upsample':\n",
      "<ipython-input-17-53a86e708621>:173: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'max_pool2d':\n",
      "<ipython-input-17-53a86e708621>:175: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif name is 'avg_pool2d':\n"
     ]
    }
   ],
   "source": [
    "class Learner(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, imgc, imgsz):\n",
    "        \"\"\"\n",
    "\n",
    "        :param config: network config file, type:list of (string, list)\n",
    "        :param imgc: 1 or 3\n",
    "        :param imgsz:  28 or 84\n",
    "        \"\"\"\n",
    "        super(Learner, self).__init__()\n",
    "\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        # this dict contains all tensors needed to be optimized\n",
    "        self.vars = nn.ParameterList()\n",
    "        # running_mean and running_var\n",
    "        self.vars_bn = nn.ParameterList()\n",
    "\n",
    "        for i, (name, param) in enumerate(self.config):\n",
    "            if name is 'conv2d':\n",
    "                # [ch_out, ch_in, kernelsz, kernelsz]\n",
    "                w = nn.Parameter(torch.ones(*param[:4]))\n",
    "                # gain=1 according to cbfin's implementation\n",
    "                torch.nn.init.kaiming_normal_(w)\n",
    "                self.vars.append(w)\n",
    "                # [ch_out]\n",
    "                self.vars.append(nn.Parameter(torch.zeros(param[0])))\n",
    "\n",
    "            elif name is 'convt2d':\n",
    "                # [ch_in, ch_out, kernelsz, kernelsz, stride, padding]\n",
    "                w = nn.Parameter(torch.ones(*param[:4]))\n",
    "                # gain=1 according to cbfin's implementation\n",
    "                torch.nn.init.kaiming_normal_(w)\n",
    "                self.vars.append(w)\n",
    "                # [ch_in, ch_out]\n",
    "                self.vars.append(nn.Parameter(torch.zeros(param[1])))\n",
    "\n",
    "            elif name is 'linear':\n",
    "                # [ch_out, ch_in]\n",
    "                w = nn.Parameter(torch.ones(*param))\n",
    "                # gain=1 according to cbfinn's implementation\n",
    "                torch.nn.init.kaiming_normal_(w)\n",
    "                self.vars.append(w)\n",
    "                # [ch_out]\n",
    "                self.vars.append(nn.Parameter(torch.zeros(param[0])))\n",
    "\n",
    "            elif name is 'bn':\n",
    "                # [ch_out]\n",
    "                w = nn.Parameter(torch.ones(param[0]))\n",
    "                self.vars.append(w)\n",
    "                # [ch_out]\n",
    "                self.vars.append(nn.Parameter(torch.zeros(param[0])))\n",
    "\n",
    "                # must set requires_grad=False\n",
    "                running_mean = nn.Parameter(torch.zeros(param[0]), requires_grad=False)\n",
    "                running_var = nn.Parameter(torch.ones(param[0]), requires_grad=False)\n",
    "                self.vars_bn.extend([running_mean, running_var])\n",
    "\n",
    "\n",
    "            elif name in ['tanh', 'relu', 'upsample', 'avg_pool2d', 'max_pool2d',\n",
    "                          'flatten', 'reshape', 'leakyrelu', 'sigmoid']:\n",
    "                continue\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def extra_repr(self):\n",
    "        info = ''\n",
    "\n",
    "        for name, param in self.config:\n",
    "            if name is 'conv2d':\n",
    "                tmp = 'conv2d:(ch_in:%d, ch_out:%d, k:%dx%d, stride:%d, padding:%d)'\\\n",
    "                      %(param[1], param[0], param[2], param[3], param[4], param[5],)\n",
    "                info += tmp + '\\n'\n",
    "\n",
    "            elif name is 'convt2d':\n",
    "                tmp = 'convTranspose2d:(ch_in:%d, ch_out:%d, k:%dx%d, stride:%d, padding:%d)'\\\n",
    "                      %(param[0], param[1], param[2], param[3], param[4], param[5],)\n",
    "                info += tmp + '\\n'\n",
    "\n",
    "            elif name is 'linear':\n",
    "                tmp = 'linear:(in:%d, out:%d)'%(param[1], param[0])\n",
    "                info += tmp + '\\n'\n",
    "\n",
    "            elif name is 'leakyrelu':\n",
    "                tmp = 'leakyrelu:(slope:%f)'%(param[0])\n",
    "                info += tmp + '\\n'\n",
    "\n",
    "\n",
    "            elif name is 'avg_pool2d':\n",
    "                tmp = 'avg_pool2d:(k:%d, stride:%d, padding:%d)'%(param[0], param[1], param[2])\n",
    "                info += tmp + '\\n'\n",
    "            elif name is 'max_pool2d':\n",
    "                tmp = 'max_pool2d:(k:%d, stride:%d, padding:%d)'%(param[0], param[1], param[2])\n",
    "                info += tmp + '\\n'\n",
    "            elif name in ['flatten', 'tanh', 'relu', 'upsample', 'reshape', 'sigmoid', 'use_logits', 'bn']:\n",
    "                tmp = name + ':' + str(tuple(param))\n",
    "                info += tmp + '\\n'\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        return info\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, vars=None, bn_training=True):\n",
    "        \"\"\"\n",
    "        This function can be called by finetunning, however, in finetunning, we dont wish to update\n",
    "        running_mean/running_var. Thought weights/bias of bn is updated, it has been separated by fast_weights.\n",
    "        Indeed, to not update running_mean/running_var, we need set update_bn_statistics=False\n",
    "        but weight/bias will be updated and not dirty initial theta parameters via fast_weiths.\n",
    "        :param x: [b, 1, 28, 28]\n",
    "        :param vars:\n",
    "        :param bn_training: set False to not update\n",
    "        :return: x, loss, likelihood, kld\n",
    "        \"\"\"\n",
    "\n",
    "        if vars is None:\n",
    "            vars = self.vars\n",
    "\n",
    "        idx = 0\n",
    "        bn_idx = 0\n",
    "\n",
    "        for name, param in self.config:\n",
    "            if name is 'conv2d':\n",
    "                w, b = vars[idx], vars[idx + 1]\n",
    "                # remember to keep synchrozied of forward_encoder and forward_decoder!\n",
    "                x = F.conv2d(x, w, b, stride=param[4], padding=param[5])\n",
    "                idx += 2\n",
    "                # print(name, param, '\\tout:', x.shape)\n",
    "            elif name is 'convt2d':\n",
    "                w, b = vars[idx], vars[idx + 1]\n",
    "                # remember to keep synchrozied of forward_encoder and forward_decoder!\n",
    "                x = F.conv_transpose2d(x, w, b, stride=param[4], padding=param[5])\n",
    "                idx += 2\n",
    "                # print(name, param, '\\tout:', x.shape)\n",
    "            elif name is 'linear':\n",
    "                w, b = vars[idx], vars[idx + 1]\n",
    "                x = F.linear(x, w, b)\n",
    "                idx += 2\n",
    "                # print('forward:', idx, x.norm().item())\n",
    "            elif name is 'bn':\n",
    "                w, b = vars[idx], vars[idx + 1]\n",
    "                running_mean, running_var = self.vars_bn[bn_idx], self.vars_bn[bn_idx+1]\n",
    "                x = F.batch_norm(x, running_mean, running_var, weight=w, bias=b, training=bn_training)\n",
    "                idx += 2\n",
    "                bn_idx += 2\n",
    "\n",
    "            elif name is 'flatten':\n",
    "                # print(x.shape)\n",
    "                x = x.view(x.size(0), -1)\n",
    "            elif name is 'reshape':\n",
    "                # [b, 8] => [b, 2, 2, 2]\n",
    "                x = x.view(x.size(0), *param)\n",
    "            elif name is 'relu':\n",
    "                x = F.relu(x, inplace=param[0])\n",
    "            elif name is 'leakyrelu':\n",
    "                x = F.leaky_relu(x, negative_slope=param[0], inplace=param[1])\n",
    "            elif name is 'tanh':\n",
    "                x = F.tanh(x)\n",
    "            elif name is 'sigmoid':\n",
    "                x = torch.sigmoid(x)\n",
    "            elif name is 'upsample':\n",
    "                x = F.upsample_nearest(x, scale_factor=param[0])\n",
    "            elif name is 'max_pool2d':\n",
    "                x = F.max_pool2d(x, param[0], param[1], param[2])\n",
    "            elif name is 'avg_pool2d':\n",
    "                x = F.avg_pool2d(x, param[0], param[1], param[2])\n",
    "\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        # make sure variable is used properly\n",
    "        assert idx == len(vars)\n",
    "        assert bn_idx == len(self.vars_bn)\n",
    "\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def zero_grad(self, vars=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param vars:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            if vars is None:\n",
    "                for p in self.vars:\n",
    "                    if p.grad is not None:\n",
    "                        p.grad.zero_()\n",
    "            else:\n",
    "                for p in vars:\n",
    "                    if p.grad is not None:\n",
    "                        p.grad.zero_()\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        override this function since initial parameters will return with a generator.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_way = 5\n",
    "miniImgNet_config = [\n",
    "        ('conv2d', [32, 3, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [32, 32, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [32, 32, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 2, 0]),\n",
    "        ('conv2d', [32, 32, 3, 3, 1, 0]),\n",
    "        ('relu', [True]),\n",
    "        ('bn', [32]),\n",
    "        ('max_pool2d', [2, 1, 0]),\n",
    "        ('flatten', []),\n",
    "        ('linear', [n_way, 32 * 5 * 5])\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torch\n",
    "from    torch import nn\n",
    "from    torch import optim\n",
    "from    torch.nn import functional as F\n",
    "from    torch.utils.data import TensorDataset, DataLoader\n",
    "from    torch import optim\n",
    "import  numpy as np\n",
    "\n",
    "# from    learner import Learner\n",
    "from    copy import deepcopy\n",
    "\n",
    "\n",
    "\n",
    "class Meta(nn.Module):\n",
    "    \"\"\"\n",
    "    Meta Learner\n",
    "    \"\"\"\n",
    "    def __init__(self,config):\n",
    "        \"\"\"\n",
    "\n",
    "        :param args:\n",
    "        \"\"\"\n",
    "        super(Meta, self).__init__()\n",
    "\n",
    "        self.update_lr = 0.01#args.update_lr\n",
    "        self.meta_lr = 0.001#args.meta_lr\n",
    "        self.n_way = 5#args.n_way\n",
    "        self.k_spt = 5#args.k_spt\n",
    "        self.k_qry = 5#args.k_qry\n",
    "        self.task_num = 4#args.task_num\n",
    "        self.update_step = 5#args.update_step\n",
    "        self.update_step_test = 10#args.update_step_test\n",
    "\n",
    "\n",
    "        self.net = Learner(config, 3, 84)\n",
    "        self.meta_optim = optim.Adam(self.net.parameters(), lr=self.meta_lr)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def clip_grad_by_norm_(self, grad, max_norm):\n",
    "        \"\"\"\n",
    "        in-place gradient clipping.\n",
    "        :param grad: list of gradients\n",
    "        :param max_norm: maximum norm allowable\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        total_norm = 0\n",
    "        counter = 0\n",
    "        for g in grad:\n",
    "            param_norm = g.data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "            counter += 1\n",
    "        total_norm = total_norm ** (1. / 2)\n",
    "\n",
    "        clip_coef = max_norm / (total_norm + 1e-6)\n",
    "        if clip_coef < 1:\n",
    "            for g in grad:\n",
    "                g.data.mul_(clip_coef)\n",
    "\n",
    "        return total_norm/counter\n",
    "\n",
    "\n",
    "    def forward(self, x_spt, y_spt, x_qry, y_qry):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x_spt:   [b, setsz, c_, h, w]\n",
    "        :param y_spt:   [b, setsz]\n",
    "        :param x_qry:   [b, querysz, c_, h, w]\n",
    "        :param y_qry:   [b, querysz]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        task_num, setsz, c_, h, w = x_spt.size()\n",
    "        querysz = x_qry.size(1)\n",
    "\n",
    "        losses_q = [0 for _ in range(self.update_step + 1)]  # losses_q[i] is the loss on step i\n",
    "        corrects = [0 for _ in range(self.update_step + 1)]\n",
    "\n",
    "\n",
    "        for i in range(task_num):\n",
    "\n",
    "            # 1. run the i-th task and compute loss for k=0\n",
    "            logits = self.net(x_spt[i], vars=None, bn_training=True)\n",
    "            loss = F.cross_entropy(logits, y_spt[i])\n",
    "            grad = torch.autograd.grad(loss, self.net.parameters())\n",
    "            fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, self.net.parameters())))\n",
    "\n",
    "            # this is the loss and accuracy before first update\n",
    "            with torch.no_grad():\n",
    "                # [setsz, nway]\n",
    "                logits_q = self.net(x_qry[i], self.net.parameters(), bn_training=True)\n",
    "                loss_q = F.cross_entropy(logits_q, y_qry[i])\n",
    "                losses_q[0] += loss_q\n",
    "\n",
    "                pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "                correct = torch.eq(pred_q, y_qry[i]).sum().item()\n",
    "                corrects[0] = corrects[0] + correct\n",
    "\n",
    "            # this is the loss and accuracy after the first update\n",
    "            with torch.no_grad():\n",
    "                # [setsz, nway]\n",
    "                logits_q = self.net(x_qry[i], fast_weights, bn_training=True)\n",
    "                loss_q = F.cross_entropy(logits_q, y_qry[i])\n",
    "                losses_q[1] += loss_q\n",
    "                # [setsz]\n",
    "                pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "                correct = torch.eq(pred_q, y_qry[i]).sum().item()\n",
    "                corrects[1] = corrects[1] + correct\n",
    "\n",
    "            for k in range(1, self.update_step):\n",
    "                # 1. run the i-th task and compute loss for k=1~K-1\n",
    "                logits = self.net(x_spt[i], fast_weights, bn_training=True)\n",
    "                loss = F.cross_entropy(logits, y_spt[i])\n",
    "                # 2. compute grad on theta_pi\n",
    "                grad = torch.autograd.grad(loss, fast_weights)\n",
    "                # 3. theta_pi = theta_pi - train_lr * grad\n",
    "                fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, fast_weights)))\n",
    "\n",
    "                logits_q = self.net(x_qry[i], fast_weights, bn_training=True)\n",
    "                # loss_q will be overwritten and just keep the loss_q on last update step.\n",
    "                loss_q = F.cross_entropy(logits_q, y_qry[i])\n",
    "                losses_q[k + 1] += loss_q\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "                    correct = torch.eq(pred_q, y_qry[i]).sum().item()  # convert to numpy\n",
    "                    corrects[k + 1] = corrects[k + 1] + correct\n",
    "\n",
    "\n",
    "\n",
    "        # end of all tasks\n",
    "        # sum over all losses on query set across all tasks\n",
    "        loss_q = losses_q[-1] / task_num\n",
    "\n",
    "        # optimize theta parameters\n",
    "        self.meta_optim.zero_grad()\n",
    "        loss_q.backward()\n",
    "        # print('meta update')\n",
    "        # for p in self.net.parameters()[:5]:\n",
    "        # \tprint(torch.norm(p).item())\n",
    "        self.meta_optim.step()\n",
    "\n",
    "\n",
    "        accs = np.array(corrects) / (querysz * task_num)\n",
    "\n",
    "        return accs\n",
    "\n",
    "\n",
    "    def finetunning(self, x_spt, y_spt, x_qry, y_qry):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x_spt:   [setsz, c_, h, w]\n",
    "        :param y_spt:   [setsz]\n",
    "        :param x_qry:   [querysz, c_, h, w]\n",
    "        :param y_qry:   [querysz]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert len(x_spt.shape) == 4\n",
    "\n",
    "        querysz = x_qry.size(0)\n",
    "\n",
    "        corrects = [0 for _ in range(self.update_step_test + 1)]\n",
    "\n",
    "        # in order to not ruin the state of running_mean/variance and bn_weight/bias\n",
    "        # we finetunning on the copied model instead of self.net\n",
    "        net = deepcopy(self.net)\n",
    "\n",
    "        # 1. run the i-th task and compute loss for k=0\n",
    "        logits = net(x_spt)\n",
    "        loss = F.cross_entropy(logits, y_spt)\n",
    "        grad = torch.autograd.grad(loss, net.parameters())\n",
    "        fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, net.parameters())))\n",
    "\n",
    "        # this is the loss and accuracy before first update\n",
    "        with torch.no_grad():\n",
    "            # [setsz, nway]\n",
    "            logits_q = net(x_qry, net.parameters(), bn_training=True)\n",
    "            # [setsz]\n",
    "            pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "            # scalar\n",
    "            correct = torch.eq(pred_q, y_qry).sum().item()\n",
    "            corrects[0] = corrects[0] + correct\n",
    "\n",
    "        # this is the loss and accuracy after the first update\n",
    "        with torch.no_grad():\n",
    "            # [setsz, nway]\n",
    "            logits_q = net(x_qry, fast_weights, bn_training=True)\n",
    "            # [setsz]\n",
    "            pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "            # scalar\n",
    "            correct = torch.eq(pred_q, y_qry).sum().item()\n",
    "            corrects[1] = corrects[1] + correct\n",
    "\n",
    "        for k in range(1, self.update_step_test):\n",
    "            # 1. run the i-th task and compute loss for k=1~K-1\n",
    "            logits = net(x_spt, fast_weights, bn_training=True)\n",
    "            loss = F.cross_entropy(logits, y_spt)\n",
    "            # 2. compute grad on theta_pi\n",
    "            grad = torch.autograd.grad(loss, fast_weights)\n",
    "            # 3. theta_pi = theta_pi - train_lr * grad\n",
    "            fast_weights = list(map(lambda p: p[1] - self.update_lr * p[0], zip(grad, fast_weights)))\n",
    "\n",
    "            logits_q = net(x_qry, fast_weights, bn_training=True)\n",
    "            # loss_q will be overwritten and just keep the loss_q on last update step.\n",
    "            loss_q = F.cross_entropy(logits_q, y_qry)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "                correct = torch.eq(pred_q, y_qry).sum().item()  # convert to numpy\n",
    "                corrects[k + 1] = corrects[k + 1] + correct\n",
    "\n",
    "\n",
    "        del net\n",
    "\n",
    "        accs = np.array(corrects) / querysz\n",
    "\n",
    "        return accs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    pass\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here start Poisoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Learner(miniImgNet_config,3,84)\n",
    "maml = Meta(miniImgNet_config)\n",
    "model.load_state_dict(torch.load(f\"model_test_empty\",map_location=torch.device('cpu')))\n",
    "maml.net = model\n",
    "# model_small.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle DB :test, b:100, 5-way, 5-shot, 15-query, resize:84\n"
     ]
    }
   ],
   "source": [
    "mini_test = MiniImagenet('miniimagenet/', mode='test', n_way=5, k_shot=5,\n",
    "                         k_query=15,\n",
    "                         batchsz=100, resize=84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :  0.2\n",
      "2 :  0.13333333333333333\n",
      "3 :  0.29333333333333333\n",
      "4 :  0.37333333333333335\n",
      "5 :  0.32\n",
      "6 :  0.32\n",
      "7 :  0.30666666666666664\n",
      "8 :  0.29333333333333333\n",
      "9 :  0.17333333333333334\n",
      "10 :  0.29333333333333333\n",
      "11 :  0.26666666666666666\n",
      "12 :  0.52\n",
      "13 :  0.3333333333333333\n",
      "14 :  0.32\n",
      "15 :  0.3333333333333333\n",
      "16 :  0.25333333333333335\n",
      "17 :  0.2\n",
      "18 :  0.30666666666666664\n",
      "19 :  0.30666666666666664\n",
      "20 :  0.28\n",
      "21 :  0.25333333333333335\n",
      "22 :  0.26666666666666666\n",
      "23 :  0.32\n",
      "24 :  0.32\n",
      "25 :  0.28\n",
      "26 :  0.3333333333333333\n",
      "27 :  0.22666666666666666\n",
      "28 :  0.22666666666666666\n",
      "29 :  0.28\n",
      "30 :  0.30666666666666664\n",
      "31 :  0.22666666666666666\n",
      "32 :  0.32\n",
      "33 :  0.25333333333333335\n",
      "34 :  0.3333333333333333\n",
      "35 :  0.30666666666666664\n",
      "36 :  0.3466666666666667\n",
      "37 :  0.22666666666666666\n",
      "38 :  0.29333333333333333\n",
      "39 :  0.3333333333333333\n",
      "40 :  0.3333333333333333\n",
      "41 :  0.24\n",
      "42 :  0.29333333333333333\n",
      "43 :  0.26666666666666666\n",
      "44 :  0.49333333333333335\n",
      "45 :  0.4266666666666667\n",
      "46 :  0.28\n",
      "47 :  0.29333333333333333\n",
      "48 :  0.38666666666666666\n",
      "49 :  0.2\n",
      "50 :  0.32\n",
      "51 :  0.22666666666666666\n",
      "52 :  0.32\n",
      "53 :  0.24\n",
      "54 :  0.26666666666666666\n",
      "55 :  0.32\n",
      "56 :  0.36\n",
      "57 :  0.37333333333333335\n",
      "58 :  0.3333333333333333\n",
      "59 :  0.32\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-9d13836ed0e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m############## ADD POISONS TO SUPPORT HERE ##############\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m############## TODO FIGURE OUT HOW TO CHECK ##############\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0maccs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinetunning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_spt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_spt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_qry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_qry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0maccs_all_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-ea9121c66c29>\u001b[0m in \u001b[0;36mfinetunning\u001b[0;34m(self, x_spt, y_spt, x_qry, y_qry)\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0mfast_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_lr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfast_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mlogits_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_qry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfast_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbn_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m             \u001b[0;31m# loss_q will be overwritten and just keep the loss_q on last update step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mloss_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_qry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-53a86e708621>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, vars, bn_training)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# remember to keep synchrozied of forward_encoder and forward_decoder!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0;31m# print(name, param, '\\tout:', x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "db_test = DataLoader(mini_test, 1, shuffle=True, num_workers=0, pin_memory=True)\n",
    "accs_all_test = []\n",
    "i = 0\n",
    "for x_spt, y_spt, x_qry, y_qry in db_test:\n",
    "    x_spt, y_spt, x_qry, y_qry = x_spt.squeeze(0), y_spt.squeeze(0), \\\n",
    "                                 x_qry.squeeze(0), y_qry.squeeze(0)\n",
    "    ############## ADD POISONS TO SUPPORT HERE ##############\n",
    "\n",
    "    accs = maml.finetunning(x_spt, y_spt, x_qry, y_qry)\n",
    "    accs_all_test.append(accs)\n",
    "    i += 1\n",
    "    print(i, \": \", accs[-1])\n",
    "accs = np.array(accs_all_test).mean(axis=0).astype(np.float16)\n",
    "print(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18666667 0.32       0.30666667 0.30666667 0.29333333 0.28\n",
      " 0.28       0.28       0.28       0.26666667 0.26666667]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "from statistics import mean, stdev\n",
    "from copy import deepcopy\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "## tqdm for loading bars\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "## Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR100, SVHN\n",
    "import torchvision.transforms as transforms\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Team Libraries\n",
    "# Needed in order to use modules from a directory level higher\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from meta_utils import *\n",
    "from Learner import *\n",
    "from learner_config import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR_train_set = CIFAR100(root=\"../data\", train=True, download=True, transform=transforms.ToTensor())\n",
    "CIFAR_test_set = CIFAR100(root=\"../data\", train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR_all_images = np.concatenate([CIFAR_train_set.data, CIFAR_test_set.data], axis=0)\n",
    "CIFAR_all_targets = torch.LongTensor(CIFAR_train_set.targets + CIFAR_test_set.targets)\n",
    "print(CIFAR_all_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CIFAR_all_images[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_IMAGES = 12\n",
    "CIFAR_images = torch.stack([CIFAR_train_set[np.random.randint(len(CIFAR_train_set))][0] for idx in range(NUM_IMAGES)], dim=0)\n",
    "img_grid = torchvision.utils.make_grid(CIFAR_images, nrow=6, normalize=True, pad_value=0.9)\n",
    "img_grid = img_grid.permute(1, 2, 0)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.title(\"Image examples of the CIFAR100 dataset\")\n",
    "plt.imshow(img_grid)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)           # Set seed for reproducibility\n",
    "classes = torch.randperm(100)  # Returns random permutation of numbers 0 to 99\n",
    "train_classes, test_classes, val_classes = classes[:64], classes[64:80], classes[80:] #similar to miniImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_class = {val: key for key, val in CIFAR_train_set.class_to_idx.items()}\n",
    "print(\"Test classes:\", [idx_to_class[c.item()] for c in val_classes])\n",
    "print(\"Val classes:\", [idx_to_class[c.item()] for c in test_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a task from dataset randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_classes = createClassSplits(CIFAR_all_images, CIFAR_all_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = train_tasks[0]['support'][0][0]\n",
    "# print(image_data)\n",
    "plt.imshow(image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "model = Learner(config_small,3,32)\n",
    "meta_iter = 10000\n",
    "meta_lr = 1e-3\n",
    "meta_opt = torch.optim.SGD(model.parameters(), lr = 1e-3)\n",
    "\n",
    "train_by_classes = data_by_classes[:64]\n",
    "test_by_classes = data_by_classes[64:85]\n",
    "val_by_classes = data_by_classes[85:]\n",
    "\n",
    "for i in range(meta_iter):    \n",
    "    outer_loss = 0\n",
    "    num_tasks = 4\n",
    "    for task in range(num_tasks):\n",
    "        task_ss, task_q = sampleTasks(train_by_classes, 5, 5)\n",
    "    \n",
    "        train_dataloader = DataLoader(task_ss, batch_size = len(task_ss), shuffle = True)\n",
    "        test_dataloader = DataLoader(task_q, batch_size = len(task_q), shuffle = True)\n",
    "\n",
    "        train_inputs, train_targets = iter(train_dataloader).next()\n",
    "        train_classes_map = torch.unique(train_targets).tolist()\n",
    "        map_ = {x: i for i, x in enumerate(train_classes_map)}\n",
    "        \n",
    "        train_targets = torch.tensor([map_[x.item()] for x in train_targets])\n",
    "        train_inputs = torch.transpose(train_inputs,1,3).type(torch.FloatTensor)\n",
    "        \n",
    "        test_inputs, test_targets = iter(test_dataloader).next()\n",
    "        \n",
    "        test_inputs = torch.transpose(test_inputs,1,3).type(torch.FloatTensor)\n",
    "        test_targets = torch.tensor([map_[x.item()] for x in test_targets])\n",
    "\n",
    "        train_logits = model(train_inputs, bn_training=True)\n",
    "        inner_loss = F.cross_entropy(train_logits, train_targets)\n",
    "\n",
    "        model.zero_grad()\n",
    "        grad = torch.autograd.grad(inner_loss, model.parameters())\n",
    "        fast_weights = list(map(lambda p: p[1] - meta_lr * p[0], zip(grad, model.parameters())))\n",
    "\n",
    "        test_logits = model(test_inputs, fast_weights, bn_training=True)\n",
    "        outer_loss += F.cross_entropy(test_logits, test_targets)\n",
    "        \n",
    "    if(i % 500 ==0): print(f\"Outer_loss at {i}: \",outer_loss/5)\n",
    "        \n",
    "    meta_opt.zero_grad()\n",
    "    outer_loss.backward()\n",
    "    meta_opt.step()\n",
    "    \n",
    "print(datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "print(i)\n",
    "model_ft_1 = copy.deepcopy(model)\n",
    "torch.save(model.state_dict(), f\"MAML_CIFAR_FS_5shot1way_{i+1}MetaIter_Q3_SmallCNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_big = Learner(config_big,3,32)\n",
    "# model_big.load_state_dict(torch.load(\"MAML_CIFAR_FS_5000MetaIter_Q15\"))\n",
    "# # model_big = torch.load(\"MAML_CIFAR_FS_5000MetaIter_Q15\")\n",
    "# model_big.train()\n",
    "model_small = Learner(config_small,3,32)\n",
    "model_small.load_state_dict(torch.load(f\"MAML_CIFAR_FS_5shot1way_10000MetaIter_Q3_SmallCNN\"))\n",
    "# model_big = torch.load(\"MAML_CIFAR_FS_5000MetaIter_Q15\")\n",
    "model_small.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tune with Ten Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "total_acc = 0\n",
    "test_num = 100\n",
    "for i in range(test_num):\n",
    "    querysz = 3\n",
    "    update_step_test = 10\n",
    "    corrects = [0 for _ in range(update_step_test + 1)]\n",
    "    update_lr = 1e-1\n",
    "    model_ft_2 = copy.deepcopy(model)\n",
    "    \n",
    "    task_ss, task_q = sampleTasks(val_by_classes, 1, 5)\n",
    "    train_dataloader = DataLoader(task_ss, batch_size = 5, shuffle = True)\n",
    "    test_dataloader = DataLoader(task_q, batch_size = querysz, shuffle = True)\n",
    "\n",
    "    train_inputs, train_targets = iter(train_dataloader).next()\n",
    "    train_classes_map = torch.unique(train_targets).tolist()\n",
    "    map_ = {x: i for i, x in enumerate(train_classes_map)}\n",
    "    train_targets = torch.tensor([map_[x.item()] for x in train_targets])\n",
    "    train_inputs = torch.transpose(train_inputs,1,3).type(torch.FloatTensor)\n",
    "\n",
    "    test_inputs, test_targets = iter(test_dataloader).next()\n",
    "    test_inputs = torch.transpose(test_inputs,1,3).type(torch.FloatTensor)\n",
    "    test_targets = torch.tensor([map_[x.item()] for x in test_targets])\n",
    "#     print(test_targets)\n",
    "    logits = model_ft_2(train_inputs, bn_training=True)\n",
    "    loss = F.cross_entropy(logits, train_targets)\n",
    "    grad = torch.autograd.grad(loss, model_ft_2.parameters())\n",
    "    fast_weights = list(map(lambda p: p[1] - update_lr * p[0], zip(grad, model_ft_2.parameters())))\n",
    "    \n",
    "    # this is the loss and accuracy before first update\n",
    "    with torch.no_grad():\n",
    "        # [setsz, nway]\n",
    "        logits_q = model_ft_2(test_inputs, model_ft_2.parameters(), bn_training=True)\n",
    "        # [setsz]\n",
    "        pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "        # scalar\n",
    "        correct = torch.eq(pred_q, test_targets).sum().item()\n",
    "        corrects[0] = corrects[0] + correct\n",
    "\n",
    "    # this is the loss and accuracy after the first update\n",
    "    with torch.no_grad():\n",
    "        # [setsz, nway]\n",
    "        logits_q = model_ft_2(test_inputs,fast_weights, bn_training=True)\n",
    "        # [setsz]\n",
    "        pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "        # scalar\n",
    "        correct = torch.eq(pred_q, test_targets).sum().item()\n",
    "        corrects[1] = corrects[1] + correct\n",
    "\n",
    "    for k in range(1, update_step_test):\n",
    "#         update_lr = 0.05\n",
    "        # 1. run the i-th task and compute loss for k=1~K-1\n",
    "        logits = model_ft_2(train_inputs, fast_weights, bn_training=True)\n",
    "        loss = F.cross_entropy(logits, train_targets)\n",
    "        # 2. compute grad on theta_pi\n",
    "        grad = torch.autograd.grad(loss, fast_weights)\n",
    "        # 3. theta_pi = theta_pi - train_lr * grad\n",
    "        fast_weights = list(map(lambda p: p[1] - update_lr * p[0], zip(grad, fast_weights)))\n",
    "#         print(loss)\n",
    "        logits_q = model_ft_2(test_inputs, fast_weights, bn_training=True)\n",
    "        # loss_q will be overwritten and just keep the loss_q on last update step.\n",
    "        loss_q = F.cross_entropy(logits_q, test_targets)\n",
    "        with torch.no_grad():\n",
    "\n",
    "#             print(loss_q)\n",
    "            pred_q = F.softmax(logits_q, dim=1).argmax(dim=1)\n",
    "            correct = torch.eq(pred_q, test_targets).sum().item()  # convert to numpy\n",
    "            corrects[k + 1] = corrects[k + 1] + correct\n",
    "\n",
    "#     print(corrects)\n",
    "    accs = np.array(corrects) / querysz\n",
    "    if(i % 50 == 0):\n",
    "        print(\"acc: \", accs[-1])\n",
    "        print(corrects)\n",
    "        print(\"loss: \", loss_q)\n",
    "    total_acc += accs[-1]/test_num\n",
    "print(total_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_q = model_ft_2(test_inputs)\n",
    "correct = (F.softmax(logits_q, dim=1).argmax(dim=1))\n",
    "print(torch.eq(correct, test_targets).sum().item())\n",
    "print(correct)\n",
    "print(test_targets)\n",
    "print(map_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(idx_to_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code that has been moved to other files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Learner.py\n",
    "\n",
    "import  torch\n",
    "from    torch import nn\n",
    "from    torch.nn import functional as F\n",
    "import  numpy as np\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# Taken From:\n",
    "\n",
    "# @misc{MAML_Pytorch,\n",
    "#   author = {Liangqu Long},\n",
    "#   title = {MAML-Pytorch Implementation},\n",
    "#   year = {2018},\n",
    "#   publisher = {GitHub},\n",
    "#   journal = {GitHub repository},\n",
    "#   howpublished = {\\url{https://github.com/dragen1860/MAML-Pytorch}},\n",
    "#   commit = {master}\n",
    "# }\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "class Learner(nn.Module):\n",
    "\n",
    "    def __init__(self, config, imgc, imgsz):\n",
    "        \"\"\"\n",
    "        :param config: network config file, type:list of (string, list)\n",
    "        :param imgc: 1 or 3\n",
    "        :param imgsz:  28 or 84\n",
    "        \"\"\"\n",
    "        super(Learner, self).__init__()\n",
    "\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        # this dict contains all tensors needed to be optimized\n",
    "        self.vars = nn.ParameterList()\n",
    "        # running_mean and running_var\n",
    "        self.vars_bn = nn.ParameterList()\n",
    "\n",
    "        for i, (name, param) in enumerate(self.config):\n",
    "            if name is 'conv2d':\n",
    "                # [ch_out, ch_in, kernelsz, kernelsz]\n",
    "                w = nn.Parameter(torch.ones(*param[:4]))\n",
    "                # gain=1 according to cbfin's implementation\n",
    "                torch.nn.init.kaiming_normal_(w)\n",
    "                self.vars.append(w)\n",
    "                # [ch_out]\n",
    "                self.vars.append(nn.Parameter(torch.zeros(param[0])))\n",
    "\n",
    "            elif name is 'convt2d':\n",
    "                # [ch_in, ch_out, kernelsz, kernelsz, stride, padding]\n",
    "                w = nn.Parameter(torch.ones(*param[:4]))\n",
    "                # gain=1 according to cbfin's implementation\n",
    "                torch.nn.init.kaiming_normal_(w)\n",
    "                self.vars.append(w)\n",
    "                # [ch_in, ch_out]\n",
    "                self.vars.append(nn.Parameter(torch.zeros(param[1])))\n",
    "\n",
    "            elif name is 'linear':\n",
    "                # [ch_out, ch_in]\n",
    "                w = nn.Parameter(torch.ones(*param))\n",
    "                # gain=1 according to cbfinn's implementation\n",
    "                torch.nn.init.kaiming_normal_(w)\n",
    "                self.vars.append(w)\n",
    "                # [ch_out]\n",
    "                self.vars.append(nn.Parameter(torch.zeros(param[0])))\n",
    "\n",
    "            elif name is 'bn':\n",
    "                # [ch_out]\n",
    "                w = nn.Parameter(torch.ones(param[0]))\n",
    "                self.vars.append(w)\n",
    "                # [ch_out]\n",
    "                self.vars.append(nn.Parameter(torch.zeros(param[0])))\n",
    "\n",
    "                # must set requires_grad=False\n",
    "                running_mean = nn.Parameter(torch.zeros(param[0]), requires_grad=False)\n",
    "                running_var = nn.Parameter(torch.ones(param[0]), requires_grad=False)\n",
    "                self.vars_bn.extend([running_mean, running_var])\n",
    "\n",
    "\n",
    "            elif name in ['tanh', 'relu', 'upsample', 'avg_pool2d', 'max_pool2d',\n",
    "                          'flatten', 'reshape', 'leakyrelu', 'sigmoid']:\n",
    "                continue\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def extra_repr(self):\n",
    "        info = ''\n",
    "\n",
    "        for name, param in self.config:\n",
    "            if name is 'conv2d':\n",
    "                tmp = 'conv2d:(ch_in:%d, ch_out:%d, k:%dx%d, stride:%d, padding:%d)'\\\n",
    "                      %(param[1], param[0], param[2], param[3], param[4], param[5],)\n",
    "                info += tmp + '\\n'\n",
    "\n",
    "            elif name is 'convt2d':\n",
    "                tmp = 'convTranspose2d:(ch_in:%d, ch_out:%d, k:%dx%d, stride:%d, padding:%d)'\\\n",
    "                      %(param[0], param[1], param[2], param[3], param[4], param[5],)\n",
    "                info += tmp + '\\n'\n",
    "\n",
    "            elif name is 'linear':\n",
    "                tmp = 'linear:(in:%d, out:%d)'%(param[1], param[0])\n",
    "                info += tmp + '\\n'\n",
    "\n",
    "            elif name is 'leakyrelu':\n",
    "                tmp = 'leakyrelu:(slope:%f)'%(param[0])\n",
    "                info += tmp + '\\n'\n",
    "\n",
    "\n",
    "            elif name is 'avg_pool2d':\n",
    "                tmp = 'avg_pool2d:(k:%d, stride:%d, padding:%d)'%(param[0], param[1], param[2])\n",
    "                info += tmp + '\\n'\n",
    "            elif name is 'max_pool2d':\n",
    "                tmp = 'max_pool2d:(k:%d, stride:%d, padding:%d)'%(param[0], param[1], param[2])\n",
    "                info += tmp + '\\n'\n",
    "            elif name in ['flatten', 'tanh', 'relu', 'upsample', 'reshape', 'sigmoid', 'use_logits', 'bn']:\n",
    "                tmp = name + ':' + str(tuple(param))\n",
    "                info += tmp + '\\n'\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        return info\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, vars=None, bn_training=True):\n",
    "        \"\"\"\n",
    "        This function can be called by finetunning, however, in finetunning, we dont wish to update\n",
    "        running_mean/running_var. Thought weights/bias of bn is updated, it has been separated by fast_weights.\n",
    "        Indeed, to not update running_mean/running_var, we need set update_bn_statistics=False\n",
    "        but weight/bias will be updated and not dirty initial theta parameters via fast_weiths.\n",
    "        :param x: [b, 1, 28, 28]\n",
    "        :param vars:\n",
    "        :param bn_training: set False to not update\n",
    "        :return: x, loss, likelihood, kld\n",
    "        \"\"\"\n",
    "\n",
    "        if vars is None:\n",
    "            vars = self.vars\n",
    "\n",
    "        idx = 0\n",
    "        bn_idx = 0\n",
    "\n",
    "        for name, param in self.config:\n",
    "            if name is 'conv2d':\n",
    "                w, b = vars[idx], vars[idx + 1]\n",
    "                # remember to keep synchrozied of forward_encoder and forward_decoder!\n",
    "                x = F.conv2d(x, w, b, stride=param[4], padding=param[5])\n",
    "                idx += 2\n",
    "                # print(name, param, '\\tout:', x.shape)\n",
    "            elif name is 'convt2d':\n",
    "                w, b = vars[idx], vars[idx + 1]\n",
    "                # remember to keep synchrozied of forward_encoder and forward_decoder!\n",
    "                x = F.conv_transpose2d(x, w, b, stride=param[4], padding=param[5])\n",
    "                idx += 2\n",
    "                # print(name, param, '\\tout:', x.shape)\n",
    "            elif name is 'linear':\n",
    "                w, b = vars[idx], vars[idx + 1]\n",
    "                x = F.linear(x, w, b)\n",
    "                idx += 2\n",
    "                # print('forward:', idx, x.norm().item())\n",
    "            elif name is 'bn':\n",
    "                w, b = vars[idx], vars[idx + 1]\n",
    "                running_mean, running_var = self.vars_bn[bn_idx], self.vars_bn[bn_idx+1]\n",
    "                x = F.batch_norm(x, running_mean, running_var, weight=w, bias=b, training=bn_training)\n",
    "                idx += 2\n",
    "                bn_idx += 2\n",
    "\n",
    "            elif name is 'flatten':\n",
    "                # print(x.shape)\n",
    "                x = x.view(x.size(0), -1)\n",
    "            elif name is 'reshape':\n",
    "                # [b, 8] => [b, 2, 2, 2]\n",
    "                x = x.view(x.size(0), *param)\n",
    "            elif name is 'relu':\n",
    "                x = F.relu(x, inplace=param[0])\n",
    "            elif name is 'leakyrelu':\n",
    "                x = F.leaky_relu(x, negative_slope=param[0], inplace=param[1])\n",
    "            elif name is 'tanh':\n",
    "                x = F.tanh(x)\n",
    "            elif name is 'sigmoid':\n",
    "                x = torch.sigmoid(x)\n",
    "            elif name is 'upsample':\n",
    "                x = F.upsample_nearest(x, scale_factor=param[0])\n",
    "            elif name is 'max_pool2d':\n",
    "                x = F.max_pool2d(x, param[0], param[1], param[2])\n",
    "            elif name is 'avg_pool2d':\n",
    "                x = F.avg_pool2d(x, param[0], param[1], param[2])\n",
    "\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        # make sure variable is used properly\n",
    "        assert idx == len(vars)\n",
    "        assert bn_idx == len(self.vars_bn)\n",
    "\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def zero_grad(self, vars=None):\n",
    "        \"\"\"\n",
    "        :param vars:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            if vars is None:\n",
    "                for p in self.vars:\n",
    "                    if p.grad is not None:\n",
    "                        p.grad.zero_()\n",
    "            else:\n",
    "                for p in vars:\n",
    "                    if p.grad is not None:\n",
    "                        p.grad.zero_()\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        override this function since initial parameters will return with a generator.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In learner_config.py\n",
    "n_ways = 5\n",
    "# [ch_in, ch_out, kernelsz, kernelsz, stride, padding] 256, 128, 64, 64\n",
    "\n",
    "config_small = [\n",
    "    ('conv2d', [32, 3, 3, 3, 2, 0]),\n",
    "    ('relu', [True]),\n",
    "    ('bn', [32]),\n",
    "    ('conv2d', [32, 32, 3, 3, 2, 0]),\n",
    "    ('relu', [True]),\n",
    "    ('bn', [32]),\n",
    "    ('conv2d', [32, 32, 3, 3, 2, 0]),\n",
    "    ('relu', [True]),\n",
    "    ('bn', [32]),\n",
    "    ('conv2d', [32, 32, 2, 2, 1, 0]),\n",
    "    ('relu', [True]),\n",
    "    ('bn', [32]),\n",
    "    ('flatten', []),\n",
    "    ('linear', [n_ways, 4*32])\n",
    "]\n",
    "config_big = [\n",
    "    ('conv2d', [256, 3, 3, 3, 2, 0]),\n",
    "    ('relu', [True]),\n",
    "    ('bn', [256]),\n",
    "    ('conv2d', [128, 256, 3, 3, 2, 0]),\n",
    "    ('relu', [True]),\n",
    "    ('bn', [128]),\n",
    "    ('conv2d', [64, 128, 3, 3, 2, 0]),\n",
    "    ('relu', [True]),\n",
    "    ('bn', [64]),\n",
    "    ('conv2d', [64, 64, 2, 2, 1, 0]),\n",
    "    ('relu', [True]),\n",
    "    ('bn', [64]),\n",
    "    ('flatten', []),\n",
    "    ('linear', [n_ways, 4*64])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In meta_utils.py\n",
    "\n",
    "def sampleTasks(data_by_class, k_shots, n_way):\n",
    "    task_extra = random.sample(data_by_classes, n_way)\n",
    "    task_ss = []\n",
    "    task_q = []\n",
    "    for by_class in task_extra:\n",
    "\n",
    "        samples = random.sample(by_class, 2*k_shots)\n",
    "\n",
    "        task_ss = task_ss + samples[:k_shots]\n",
    "        task_q = task_q +samples[k_shots:k_shots+3]\n",
    "        \n",
    "    random.shuffle(task_ss)\n",
    "    random.shuffle(task_q)\n",
    "    \n",
    "    return task_ss, task_q[:3*k_shots]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In meta_utils.py\n",
    "\n",
    "def createClassSplits(all_imgs, all_labels):#, train_classes, test_classes, val_classes):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        imgs - Numpy array of shape [N,32,32,3] containing all images.\n",
    "        targets - PyTorch array of shape [N] containing all labels.\n",
    "        k-shot - \n",
    "        \n",
    "    \"\"\"\n",
    "        \n",
    "    classes = torch.unique(all_labels).tolist()\n",
    "    data_by_classes = []\n",
    "    print(classes)\n",
    "\n",
    "    for class_ in classes:\n",
    "        \n",
    "        holder_array = []\n",
    "        if(class_ % 10 == 0): print(class_)\n",
    "        idx_array = [i for i, x in enumerate(all_labels) if x == class_]\n",
    "\n",
    "        labels_for_class = [all_labels[i] for i in idx_array]\n",
    "        img_for_class = [all_imgs[i] for i in idx_array]\n",
    "        combined_by_class =[]\n",
    "\n",
    "        combined_by_class = [[img_for_class[i],labels_for_class[i]] for i in range(len(img_for_class))]\n",
    "\n",
    "        data_by_classes.append(combined_by_class)\n",
    "            \n",
    "    return data_by_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code that has been commented out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def createTasks(training_data_ss, training_data_q, n_way, k_shots):\n",
    "    \n",
    "#     no_tasks = len(training_data_ss)/n_way\n",
    "    \n",
    "#     classes = [element[0][1] for element in training_data_ss]\n",
    "\n",
    "#     c = list(zip(training_data_ss, training_data_q))\n",
    "\n",
    "#     random.shuffle(c)\n",
    "\n",
    "#     training_data_ss, training_data_q = zip(*c)\n",
    "\n",
    "#     tasks = []\n",
    "\n",
    "#     for i in range(int(len(classes)/n_way)):\n",
    "#         task_dic = {}\n",
    "#         task_holder_ss = []\n",
    "#         task_holder_q = []\n",
    "#         if(n_way*(i+1)-1 < len(classes)):   \n",
    "#             for j in range(n_way):\n",
    "#                 task_holder_ss= [*task_holder_ss, *training_data_ss[n_way*i+j]]\n",
    "#                 task_holder_q= [*task_holder_ss, *training_data_q[n_way*i+j]]\n",
    "#         else:\n",
    "#             last_index_p1 = len(classes)\n",
    "            \n",
    "#             for j in range(n_way*(i), last_index_p1):\n",
    "#                 task_holder_ss= [*task_holder_ss, *training_data_ss[n_way*i+j]]\n",
    "#                 task_holder_q= [*task_holder_ss, *training_data_q[n_way*i+j]]\n",
    "#         random.shuffle(task_holder_ss)\n",
    "#         random.shuffle(task_holder_q)\n",
    "#         task_dic['support'] = task_holder_ss\n",
    "#         task_dic['query'] = task_holder_q[:3*k_shots]\n",
    "        \n",
    "#         tasks.append(task_dic)\n",
    "    \n",
    "#     return tasks\n",
    "\n",
    "# train_tasks = createTasks(train_by_class, train_by_class_q, 5, 5)\n",
    "# test_tasks = createTasks(test_by_class, test_by_class_q, 5, 5)\n",
    "# val_tasks = createTasks(val_by_class, val_by_class_q, 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def createSplits(all_imgs, all_labels, k_shot):#, train_classes, test_classes, val_classes):\n",
    "#     \"\"\"\n",
    "#     Inputs:\n",
    "#         imgs - Numpy array of shape [N,32,32,3] containing all images.\n",
    "#         targets - PyTorch array of shape [N] containing all labels.\n",
    "#         k-shot - \n",
    "        \n",
    "#     \"\"\"\n",
    "        \n",
    "#     classes = torch.unique(all_labels).tolist()\n",
    "#     data_by_classes = []\n",
    "    \n",
    "#     #organize by classes\n",
    "#     for class_ in classes:\n",
    "        \n",
    "#         holder_array = []\n",
    "#         if(class_ % 10 == 0): print(class_)\n",
    "#         idx_array = [i for i, x in enumerate(all_labels) if x == class_]\n",
    "# #         print(idx_array)\n",
    "#         labels_for_class = [all_labels[i] for i in idx_array]\n",
    "#         img_for_class = [all_imgs[i] for i in idx_array]\n",
    "# #         print(len(img_for_class))\n",
    "#         data_by_classes.append([img_for_class,labels_for_class])\n",
    "            \n",
    "    \n",
    "#     #create tasks for train, test, and val\n",
    "#     #train_classes may need to come from a split folder\n",
    "    \n",
    "#     train_by_class = []\n",
    "#     test_by_class = []\n",
    "#     val_by_class = []\n",
    "#     train_by_class_q = []\n",
    "#     test_by_class_q = []\n",
    "#     val_by_class_q = []\n",
    "    \n",
    "    \n",
    "#     random.shuffle(classes)\n",
    "#     classes_for_tasks = classes\n",
    "    \n",
    "#     #To create Query Set \n",
    "    \n",
    "\n",
    "#     train_classes, test_classes, val_classes = classes_for_tasks[:75], classes_for_tasks[75:90], classes_for_tasks[90:]\n",
    "    \n",
    "#     for data_by_class in data_by_classes:\n",
    "        \n",
    "#         class_in_set = data_by_class[1][0]\n",
    "#         if(class_in_set in train_classes):\n",
    "#             k_shot_indicies = random.sample(range(0,len(data_by_class[0])-k_shot-1), k_shot)\n",
    "#             class_train_data = [[data_by_class[0][i], data_by_class[1][i]] for i in k_shot_indicies]\n",
    "#             query_set_class_train = [[data_by_class[0][i], data_by_class[1][-i]] for i in range(1,k_shot+1)]\n",
    "#             train_by_class_q.append(query_set_class_train)\n",
    "#             train_by_class.append(class_train_data)\n",
    "            \n",
    "#         elif(class_in_set in test_classes):\n",
    "            \n",
    "#             k_shot_indicies = random.sample(range(0,len(data_by_class[0])-k_shot-1), k_shot)\n",
    "#             class_test_data = [[data_by_class[0][i], data_by_class[1][i]] for i in k_shot_indicies]\n",
    "#             query_set_class_test = [[data_by_class[0][-i], data_by_class[1][-i]] for i in range(1,k_shot+1)]\n",
    "#             test_by_class_q.append(query_set_class_test)\n",
    "#             test_by_class.append(class_test_data)\n",
    "            \n",
    "#         elif(class_in_set in val_classes):\n",
    "            \n",
    "#             k_shot_indicies = random.sample(range(0,len(data_by_class[0])-k_shot-1), k_shot)\n",
    "#             class_val_data = [[data_by_class[0][i], data_by_class[1][i]] for i in k_shot_indicies]\n",
    "#             query_set_class_val = [[data_by_class[0][-i], data_by_class[1][-i]] for i in range(1,k_shot+1)]\n",
    "#             val_by_class_q.append(query_set_class_val)\n",
    "#             val_by_class.append(class_val_data)\n",
    "    \n",
    "#     return train_by_class, test_by_class, val_by_class,train_by_class_q, test_by_class_q, val_by_class_q\n",
    "\n",
    "# train_by_class, test_by_class, val_by_class,train_by_class_q, test_by_class_q, val_by_class_q = createSplits(CIFAR_all_images, CIFAR_all_targets, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
